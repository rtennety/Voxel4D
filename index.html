<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving">
  <meta name="keywords" content="Voxel4D, 4D Spatial Forecasting, Autonomous Driving, World Models, Forecasting, Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <!-- MathJax for rendering mathematical equations -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      }
    };
  </script>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <div class="password-notice">
        Website will need password until ISEF concludes so that it won't be falsely labeled as someone else's work.
      </div>
      <div class="password-header">
        <h2 class="title is-3">Voxel4D</h2>
        <p class="subtitle is-6">Enter password to access</p>
      </div>
      <div class="password-body">
        <div class="field">
          <div class="control has-icons-left has-icons-right">
            <input class="input is-large" type="password" id="password-input" placeholder="Enter password" autocomplete="off" autofocus>
            <span class="icon is-small is-left">
              <i class="fas fa-lock"></i>
            </span>
            <span class="icon is-small is-right password-toggle-icon" id="password-toggle" style="cursor: pointer; pointer-events: auto;">
              <i class="fas fa-eye" id="password-toggle-icon" style="pointer-events: none;"></i>
            </span>
          </div>
        </div>
        <div id="password-error" class="notification is-danger is-hidden" style="margin-top: 1rem; padding: 0.75rem;">
          <p style="margin: 0;">Incorrect password. Please try again.</p>
        </div>
        <div class="field" style="margin-top: 1.5rem;">
          <div class="control">
            <button class="button is-primary is-large is-fullwidth" id="password-submit">
              <span>Access Site</span>
              <span class="icon">
                <i class="fas fa-arrow-right"></i>
              </span>
            </button>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Main Content (hidden until authenticated) -->
  <div id="main-content" style="display: none;">
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/rtennety">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Voxel4D: A Unified Vision-Centric World Model for 4D Spatial Forecasting and End-to-End Planning in Autonomous Driving</h1>
          <h2 class="subtitle is-4" style="margin-top: 1rem; color: #555;">
            Using vision-centric multi-view cameras, this system predicts future 4D spatial states based on different possible driving actions, enabling real-time forecasting of how the environment will change. These predictions are integrated into an end-to-end planning system that chooses the best driving path based on forecasted spatial information, advancing autonomous driving technology.
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/rtennety">Rohan Tennety</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rtennety/Voxel4D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./assets/figures/1.png?v=2" alt="Voxel4D Teaser" style="width: 100%; max-width: 1200px;">
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <span class="dnerf">Voxel4D</span> teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Research Statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Research Statement</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.1em; font-weight: 500; word-spacing: normal; letter-spacing: normal; text-align: left !important;">
            I introduce a unified vision-centric world model framework that addresses the fundamental challenge of action-conditioned future state prediction in autonomous driving. This approach forecasts 4D spatial states conditioned on ego-vehicle actions and demonstrates how integrating these forecasts into an end-to-end planning system enables safer and more efficient autonomous navigation.
          </p>
          <p>
            In simpler terms: This system teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive. Unlike current systems that react to the present moment, this system proactively predicts multiple possible futures based on different actions, enabling safer decision-making through anticipation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Research Statement. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-driving cars need to understand not just the current environment, but how it will evolve. Current systems like Tesla process frames sequentially, reacting to the present moment rather than predicting future states. This reactive approach limits safety and efficiency.
          </p>
          <p>
            I developed <b>Voxel4D</b>, a vision-centric system that predicts 4D spatial states (3D space + time) using only camera images. The system operates through three stages: (1) <b>History Encoder</b> converts multi-view camera images into a bird's-eye view representation, (2) <b>Memory Queue</b> accumulates temporal information about object movements using semantic and motion-conditional normalization, and (3) <b>World Decoder</b> generates action-conditioned future predictions. The key innovation is that Voxel4D uses these predictions for occupancy-based trajectory planning, enabling proactive decision-making rather than reactive responses.
          </p>
          <p>
            Voxel4D processes space divided into millions of 3D voxels (512×512×40 = 10.5 million voxels per frame), meaning the system correctly predicts the occupancy of approximately 1 million voxels per frame. Given that most voxels are empty space, correctly identifying which of millions of voxels will be occupied by objects in the future is extremely challenging. This level of precision enables the fine-grained collision detection and safe navigation demonstrated by the exceptional collision rates. Voxel4D achieves state-of-the-art performance: 9.5% improvement in mIoU_f on nuScenes, 6.1% on Lyft-Level5, with real-time operation at 401ms latency. In planning evaluation, Voxel4D demonstrates exceptional safety with collision rates of 0.035% at 1 second, 0.16% at 2 seconds, and 0.493% at 3 seconds prediction horizons. This represents the first system to successfully integrate 4D occupancy forecasting with end-to-end planning for autonomous driving.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    
    <!-- 4D Spatial and Flow Forecasting -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">4D Spatial and Flow Forecasting</h2>
            <div class="content has-text-justified">
              <h3 class="title is-5" style="margin-top: 1rem; margin-bottom: 0.5rem;">What is 4D Spatial Forecasting?</h3>
              <p>
                4D Spatial Forecasting predicts which parts of 3D space will be filled by objects in the future (3D space + time). Voxel4D divides space into millions of voxels and predicts occupancy probabilities $P(\text{occupied} \mid x, y, z, t)$ for each voxel at future time steps. This enables precise collision detection and safe trajectory planning, as the system knows exactly which 3D locations will be occupied at specific future moments, not just that "there's a car somewhere ahead."
              </p>
              <p>
                Voxel4D models both moving objects (cars, pedestrians) and static parts (roads, buildings), creating a spatiotemporal representation that captures current states and predicted future states. This is fundamentally different from object detection systems that only identify present objects, enabling the system to understand how the scene will evolve dynamically over time.
              </p>
            </div>

            <!-- Scene 1 -->
            <h3 class="title is-4 has-text-centered">Scene 1 (Lane Change)</h3>
            <img src="./assets/figures/forecasting_1.png?v=6" alt="Lane Change" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_1.gif?v=6" alt="Lane Change GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 2 -->
            <h3 class="title is-4 has-text-centered">Scene 2 (Pedestrian Crossing)</h3>
            <img src="./assets/figures/forecasting_2.png?v=6" alt="Pedestrian Crossing" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_2.gif?v=6" alt="Pedestrian Crossing GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 3 -->
            <h3 class="title is-4 has-text-centered">Scene 3 (Vehicle Following)</h3>
            <img src="./assets/figures/forecasting_3.png?v=6" alt="Vehicle Following" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_3.gif?v=6" alt="Vehicle Following GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>
            
            <!-- Additional Forecasting Visualizations -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163245.png" alt="4D Occupancy Forecasting Examples" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  These visualizations show Voxel4D making predictions in real driving scenarios. The top row shows what the cameras see at different moments in time, and the bottom row shows what Voxel4D predicts will happen in the next 2 seconds. The system accurately predicts complex situations: a car changing lanes, pedestrians crossing the road, and the car following another vehicle. Most impressively, the system works even at night (shown in the third example), which is much harder for cameras than daytime. These results prove that Voxel4D can handle real-world driving conditions, not just perfect laboratory settings.
                </p>
                <p>
                  The mathematical precision of these predictions is remarkable. For each scenario, the system must predict occupancy probabilities for millions of voxels across multiple future time steps, requiring sophisticated neural network computations that process historical information, semantic understanding, and motion patterns. The accuracy demonstrated in these visualizations validates that Voxel4D's innovations, particularly semantic and motion-conditional normalization and action-controllable generation, enable the system to make highly accurate predictions even in challenging conditions like nighttime driving, where visual information is limited and object detection is more difficult.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ 4D Spatial and Flow Forecasting -->

    <!-- Continuous Forecasting and Planning -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Continuous Forecasting and Planning</h2>
            <div class="content has-text-justified">
              <p>
                Voxel4D continuously forecasts future states and uses those predictions for planning. For each possible action, it predicts what the world will look like, then chooses the safest and most efficient path using occupancy-based cost functions: $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$. The trajectory minimizing total cost is selected. Every 0.5 seconds, the system receives new camera images, updates predictions, re-evaluates trajectories, and selects optimal paths, enabling real-time adaptation to changing conditions.
              </p>
            </div>

            <!-- Scene 1 -->
            <h3 class="title is-4 has-text-centered">Scene 1 (Turn Left to Avoid Stopped Vehicle)</h3>
            <img src="./assets/figures/planning_1.png?v=2" alt="Planning Scene 1" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_1.gif?v=2" alt="Planning Scene 1 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 2 -->
            <h3 class="title is-4 has-text-centered">Scene 2 (Slowing Down to Wait for Crossing Pedestrians)</h3>
            <img src="./assets/figures/planning_2.png?v=2" alt="Planning Scene 2" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_2.gif?v=2" alt="Planning Scene 2 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 3 -->
            <h3 class="title is-4 has-text-centered">Scene 3 (Turn Right to Avoid Stopped Vehicle)</h3>
            <img src="./assets/figures/planning_3.png?v=2" alt="Planning Scene 3" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_3.gif?v=2" alt="Planning Scene 3 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>
            
            <!-- Action-Controllable Generation Visualization -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163408.png" alt="Action-Controllable Generation" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This demonstrates one of Voxel4D's most significant innovations: action-conditioned future prediction, which enables the system to generate different future scenarios based on different possible actions. The system can evaluate hypothetical scenarios such as "What if I turn left?" "What if I accelerate?" or "What if I decelerate?" The image illustrates two examples: steering angle variation and velocity variation. When predicting outcomes for high velocity scenarios, the system identifies that the vehicle would approach dangerously close to pedestrians. For low velocity scenarios, the system predicts that a safe distance would be maintained. This capability to simulate multiple future outcomes before executing actions distinguishes Voxel4D from reactive systems, as it enables safety evaluation of potential actions prior to execution.
                </p>
              </div>
            </div>
            
            <!-- Continuous Forecasting and Planning Visualization -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163416.png" alt="Continuous Forecasting and Planning" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  These visualizations demonstrate Voxel4D's planning and navigation capabilities in real-world scenarios. The top row displays camera inputs, while the bottom row shows Voxel4D's predicted future states and selected trajectories (indicated by red arrows). The results demonstrate successful performance: the system effectively avoids stopped vehicles through evasive maneuvers, maintains functionality in rainy conditions that challenge camera-based perception, and appropriately yields to pedestrians by decelerating. These examples validate that Voxel4D not only predicts future states but also utilizes those predictions to make intelligent, safe driving decisions in real-time.
                </p>
              </div>
            </div>
            
            <!-- Additional Planning Scenarios -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163427.png" alt="Planning Scenarios" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Additional examples demonstrate Voxel4D's successful planning of safe trajectories across diverse situations. The system effectively handles complex scenarios including lane changes, intersection navigation, and pedestrian interactions, all of which present significant challenges for autonomous driving systems. The consistent performance across these varied scenarios validates that Voxel4D's approach of integrating future prediction with planning generalizes well to diverse real-world conditions beyond specific test cases.
                </p>
              </div>
            </div>
            
            <!-- Additional Results Images (from upload 2 - moved here after upload 3) -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163443.png" alt="Additional Results 1" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This analysis shows Voxel4D's performance across diverse driving scenarios. Results demonstrate consistent excellent performance in cities, highways, good weather, and challenging conditions, proving robustness and reliability across diverse situations.
                </p>
              </div>
            </div>
            
            <div style="margin-top: 2rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163455.png" alt="Additional Results 2" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This table shows Voxel4D's performance in complex situations with many objects moving simultaneously (busy intersections with multiple cars, pedestrians, cyclists). Results: even in challenging scenarios, Voxel4D maintains high accuracy in predicting all object locations, proving the innovations (semantic normalization, action-controllable generation) work well in complicated situations, essential for real-world self-driving.
                </p>
              </div>
            </div>
            
            <div style="margin-top: 2rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163504.png" alt="Additional Results 3" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This evaluation tests Voxel4D's ability to predict fine-grained details (exactly which 3D voxels will be occupied, not just "there's a car somewhere"). Results show significant improvements over previous methods, with Voxel4D achieving much higher accuracy in predicting detailed spatial states. This level of detail is crucial for safe driving in busy urban environments.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ Continuous Forecasting and Planning -->

    <!-- Results: Benchmark Comparison. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">State-of-the-Art Performance</h2>
        <div class="content has-text-justified">
          <p>
            Voxel4D achieves state-of-the-art performance on major autonomous driving benchmarks, demonstrating significant improvements over previous methods. The following tables show quantitative results on the nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets, highlighting the system's superior forecasting accuracy and planning capabilities.
          </p>
        </div>
        
        <!-- Visual Demonstration: Inputs and Outputs -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163358.png" alt="Visual Inputs and Occupancy Outputs Demonstration" style="width: 100%; max-width: 1400px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This visualization demonstrates Voxel4D's end-to-end forecasting and planning pipeline. The top row shows visual inputs from cameras at -1s, -0.5s, and 0s (current moment), capturing the driving environment. The bottom row displays occupancy outputs: bird's-eye view predictions at 0.5s, 1s, 1.5s, and 2s into the future, showing predicted occupied space (vehicles, obstacles) and the ego vehicle's planned trajectory. The "Lane Change" annotation illustrates how Voxel4D uses future occupancy predictions to plan complex maneuvers proactively, demonstrating the system's ability to integrate perception, prediction, and planning into a unified real-time system.
            </p>
          </div>
        </div>
        
        <!-- Main Benchmark Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Other Methods.png" alt="Comparison with Other Methods" style="width: 100%; max-width: 1400px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This comparison shows how Voxel4D differs from other systems. Most existing systems either generate training data or work only during training, but don't help during actual driving. Voxel4D uniquely integrates world modeling (predicting the future) directly with planning (choosing the best path) in real-time. The key distinction: traditional systems treat prediction and planning as separate problems, while Voxel4D uses predicted future states to inform planning decisions, enabling proactive decision-making rather than reactive responses. This real-time integration represents a paradigm shift from systems that react to the present moment to systems that anticipate future scenarios.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Results: Benchmark Comparison. -->
    
    <!-- Comparison with Other Methods -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison with Existing Methods</h2>
        <div class="content has-text-justified">
          <p>
            Most existing autonomous driving systems fall into four categories: (1) world models used only for data generation during training, (2) world models for pretraining but not integrated into planning, (3) planning systems that use current perception without future prediction, or (4) occupancy prediction systems without planning integration. Voxel4D uniquely integrates all these capabilities into a unified system, using world modeling (predicting future states) for real-time planning during actual driving. Unlike traditional systems that treat prediction and planning as separate problems, Voxel4D uses predicted future states to inform planning decisions, enabling proactive decision-making rather than reactive responses. This real-time integration represents a paradigm shift from systems that react to the present moment to systems that anticipate future scenarios, demonstrating the advantages of integrating world modeling with end-to-end planning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Comparison with Other Methods -->
    
    <!-- Why Voxel4D is Better. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Innovations and Contributions</h2>
        <div class="content has-text-justified">
          <p style="margin-bottom: 1.5rem;">
            Current autonomous driving systems (Tesla, Waymo) face fundamental limitations. They operate on <b>reactive architectures</b> that process camera frames sequentially and react only to the present moment—like a driver who only looks at what's directly in front of them without anticipating what might happen next. They use <b>modular designs</b> where perception (identifying objects), prediction (forecasting where objects will move), and planning (choosing the vehicle's path) are separate stages that don't communicate effectively. Finally, they rely on <b>simplified bounding boxes</b> (rectangular boxes around objects) that lose critical 3D structure information. Voxel4D addresses these through three innovations: <b>proactive prediction</b> (forecasting 2-4 seconds ahead), <b>end-to-end integration</b> (perception, prediction, planning trained together as one system), <b>multi-future generation</b> (simulating different possible futures for different actions), and <b>fine-grained 3D occupancy</b> (dividing space into millions of tiny 3D cubes called voxels for precise collision detection).
          </p>
          
          <h3 class="title is-4" style="margin-top: 1.5rem; margin-bottom: 1rem;">Voxel4D's Key Innovations</h3>
          
          <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Innovation #1: Semantic and Motion-Conditional Normalization</h4>
          <p>
            To understand this innovation, we first need to understand what "BEV features" are. When Voxel4D processes camera images, it converts them into a Bird's-Eye View (BEV) representation—a top-down 2D grid where each cell contains features describing what's in that 3D location. However, traditional BEV features have a problem: features from the same 3D ray (the same physical location viewed from different camera angles) appear too similar, making it difficult to distinguish between different objects like cars, pedestrians, and buildings.
          </p>
          <p>
            Voxel4D introduces novel normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$ where $\gamma^*$ and $\beta^*$ are learned scale and bias parameters conditioned on semantic predictions (object type: car, pedestrian, etc.) and ego-motion (how the vehicle itself is moving). Normalization is a technique that adjusts feature values to make them more useful for learning. The key innovation is that these adjustments ($\gamma^*$ and $\beta^*$) are not fixed—they adapt based on what type of object is present and how the vehicle is moving. This emphasizes features corresponding to actual objects while compensating for ego-vehicle movement, separating static object motion (apparent motion due to the car moving) from dynamic object motion (actual object movement). This enhancement directly contributes to the <b>9.5% mIoU improvement</b>. This is the first method to address semantic discrimination in BEV features for occupancy forecasting.
          </p>
          
          <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Innovation #2: Action-Controllable Future Generation</h4>
          <p>
            Most prediction systems can only answer "what will happen next?" but cannot answer "what will happen if I take a specific action?" Voxel4D's world model (a system that learns to simulate future states) generates different future predictions based on different ego-vehicle actions (velocity, steering angle, trajectory waypoints). The mathematical formulation is: $\mathbf{O}_{t+1:t+T}, \mathbf{F}_{t+1:t+T} = \text{WorldDecoder}([\mathbf{H}_t, \mathbf{a}_t])$ where $\mathbf{O}$ and $\mathbf{F}$ are future occupancy and flow predictions, $\mathbf{H}_t$ represents historical features (what the system has seen), and $\mathbf{a}_t$ is the action condition (what the vehicle plans to do).
          </p>
          <p>
            This enables "what-if" scenario planning: the planner can compare multiple possible futures and select the safest option before executing any action. For example, if accelerating predicts dangerous proximity to pedestrians while decelerating maintains safe distance, the system chooses the safer action. This is fundamentally different from reactive systems that must wait to see what happens after taking an action. This is the first system to integrate flexible action conditioning into 4D occupancy forecasting world models, enabling proactive planning rather than reactive responses.
          </p>
          
          <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Innovation #3: Occupancy-Based Planning Integration</h4>
          <p>
            Most planning systems use simplified representations like bounding boxes (rectangular boxes around objects) that don't capture the full 3D structure of objects and the environment. Voxel4D uses fine-grained 3D occupancy predictions: space is divided into millions of tiny 3D cubes called voxels (each typically 0.2 meters on each side), and the system predicts which voxels will be occupied by objects in the future. This provides much more detailed spatial information than bounding boxes—the system knows not just "there's a car in this general area" but exactly which 3D locations will be filled.
          </p>
          <p>
            Candidate trajectories (possible paths the vehicle could take) are evaluated using a cost function: $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$, where $C_{\text{agent}}$ penalizes collisions with dynamic objects (moving cars, pedestrians), $C_{\text{background}}$ penalizes static object collisions (buildings, parked cars), and $C_{\text{efficiency}}$ promotes smooth, rule-compliant paths. The trajectory $\tau^* = \arg\min_{\tau} C(\tau)$ that minimizes total cost is selected. The system operates continuously: every 0.5 seconds, it receives new images, updates predictions, re-evaluates trajectories, and selects optimal paths. This enables precise voxel-level collision detection rather than bounding-box approximations. This is the first system to integrate 4D occupancy forecasting directly with end-to-end planning.
          </p>
          
          <h3 class="title is-4" style="margin-top: 2rem; margin-bottom: 1rem;">Performance & Training</h3>
          <p>
            <b>Results:</b> Voxel4D achieves state-of-the-art performance across multiple datasets. On <b>nuScenes</b> (a major autonomous driving benchmark): 36.3 mIoU_f (mean Intersection over Union for forecasting), representing a <b>+9.5 point improvement</b> over previous systems; 25.1 VPQ_f (Video Panoptic Quality for forecasting), a <b>+5.1 point improvement</b>. On <b>Lyft-Level5</b>: 39.7 mIoU_f (<b>+6.1 points</b>), 33.4 VPQ_f (<b>+5.2 points</b>). On <b>nuScenes-Occupancy</b>: <b>+4.3%</b> improvement in fine-grained occupancy prediction. The system operates at <b>401ms latency</b>, making it suitable for real-time autonomous driving.
          </p>
          <p>
            The <b>mIoU</b> (mean Intersection over Union) metric measures forecasting accuracy by comparing predicted occupied voxels with ground truth: $\text{IoU} = \frac{|\text{Predicted} \cap \text{GroundTruth}|}{|\text{Predicted} \cup \text{GroundTruth}|}$, averaged across object classes. Higher IoU means better prediction accuracy. In computer vision research, improvements of 2-3% are considered significant, and 5-10% represent major breakthroughs. Voxel4D's <b>9.5% improvement is nearly three times the breakthrough threshold</b>, demonstrating substantial impact.
          </p>
          <p>
            <b>Training:</b> Voxel4D is trained end-to-end using multi-task learning, meaning all components learn together rather than separately. The total loss function is: $L_{\text{total}} = w_1 L_{\text{occ}} + w_2 L_{\text{flow}} + w_3 L_{\text{sem}} + w_4 L_{\text{plan}}$, where $L_{\text{occ}}$ (cross-entropy loss) measures occupancy prediction accuracy, $L_{\text{flow}}$ (L1 loss) measures motion prediction accuracy, $L_{\text{sem}}$ (cross-entropy loss) measures semantic classification accuracy, and $L_{\text{plan}}$ (L2 loss) measures trajectory planning accuracy. All components (history encoder, memory queue, world decoder, planner) are optimized together through backpropagation, ensuring seamless integration rather than separate modules that might not work well together.
          </p>
        </div>
      </div>
    </div>
    <!--/ Why Voxel4D is Better. -->
    
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <img src="./assets/figures/pipeline.png?v=2" alt="Voxel4D Pipeline" style="width: 100%; max-width: 1200px;">
        <div class="content has-text-justified">
          <p>
            <b>How Voxel4D Works:</b> (a) The <b>history encoder</b> processes images from 6 cameras using transformer networks with cross-attention, converting 2D camera views into a unified bird's-eye view (200×200 grid). (b) The <b>memory queue</b> stores historical BEV embeddings (1-3 frames, 0.5s apart) and applies semantic and motion-conditional normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$. (c) The <b>world decoder</b> takes enhanced historical features and action conditions (velocity, steering, trajectory) and generates future predictions: $\mathbf{O}_{t+1:t+T}, \mathbf{F}_{t+1:t+T} = \text{WorldDecoder}([\mathbf{H}_t, \mathbf{a}_t])$, outputting occupancy and flow predictions for 2-4 seconds ahead. The planning system evaluates candidate trajectories using occupancy-based cost functions, selecting optimal paths. The system operates continuously, updating every 0.5 seconds as new camera images arrive.
          </p>
        </div>
        
        <!-- Architecture Visualization -->
        <div style="margin-top: 1rem; margin-bottom: 0.5rem;">
          <img src="./assets/figures/ResearchPaperImages/Voxel4Dmain.png" alt="Voxel4D Architecture" style="width: 100%; max-width: 1200px; display: block;">
          <div class="content has-text-justified" style="margin-top: 0.5rem;">
            <p>
              This diagram shows the complete Voxel4D system architecture. (1) History encoder: processes 6 camera images using transformer networks with cross-attention, converting to unified BEV representation (200×200 grid). (2) Memory queue: stores historical BEV embeddings, applies ego-motion compensation, and enhances features using semantic and motion-conditional normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$. (3) World decoder: takes enhanced features and action conditions, generates future occupancy and flow predictions. All components are trained end-to-end using multi-task loss: $L_{\text{total}} = w_1 L_{\text{occ}} + w_2 L_{\text{flow}} + w_3 L_{\text{sem}} + w_4 L_{\text{plan}}$, ensuring synergistic operation. This integrated architecture explains Voxel4D's superior performance.
            </p>
          </div>
        </div>
        
        <!-- Planning Performance Metrics and Methodology -->
        <div style="margin-top: 3rem;">
          <h3 class="title is-4 has-text-centered">Planning Performance: Collision Rate Evaluation Methodology</h3>
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              Voxel4D's planning performance is evaluated using two critical metrics: <b>L2 distance error</b> (measuring trajectory accuracy) and <b>collision rate</b> (measuring safety). The collision rate evaluation follows the rigorous methodology established in the Drive-OccWorld paper (Yang et al., 2024) and used throughout autonomous driving research to ensure fair comparison with state-of-the-art systems including UniAD, VAD-Base, ST-P3, and Drive-WM.
            </p>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Mathematical Formulation of Collision Rate</h4>
            <p>
              Following the methodology in Yang et al. (2024), planning evaluation uses the <b>modified collision rate</b> proposed by Li et al. (2024b), which accounts for the cumulative nature of collisions across time horizons. The collision rate at time horizon $t$ is formally defined as:
            </p>
            <div style="text-align: center; margin: 1.5rem 0;">
              $$CR(t) = \sum_{t'=0}^{N_f} \mathbb{I}_{t'} > 0$$
            </div>
            <p>
              where $\mathbb{I}_{t'}$ is an indicator function that equals 1 if the ego vehicle at timestamp $t'$ intersects with any obstacle (dynamic or static), and 0 otherwise. $N_f$ represents the total number of future time steps in the prediction horizon. This formulation ensures that if a collision occurs at any point along the trajectory, it is properly accounted for in the cumulative collision rate, reflecting the safety-critical nature of autonomous driving where a single collision is unacceptable.
            </p>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Collision Detection Algorithm</h4>
            <p>
              For each planned trajectory point $\tau_t = (x_t, y_t, z_t)$ at time $t$, the system performs voxel-level collision detection by checking whether the ego vehicle's occupancy volume intersects with predicted or ground truth object occupancy. The collision detection algorithm operates as follows:
            </p>
            <ol>
              <li><b>Trajectory Voxelization:</b> The ego vehicle's 3D bounding box (dimensions: width $W=1.85$m, length $L=4.084$m, height $H=1.5$m) is discretized into voxels within the BEV grid. The trajectory $\tau_{0:N_f}$ is transformed from world coordinates to BEV grid coordinates using the transformation: $(x_{bev}, y_{bev}) = \left(\frac{x - x_{bound}}{dx}, \frac{y - y_{bound}}{dy}\right)$, where $dx = dy = 0.2$m is the voxel resolution.</li>
              <li><b>Occupancy Intersection Check:</b> For each time step $t \in [0, N_f]$, the system checks whether any voxel $(i, j, k)$ occupied by the ego vehicle's bounding box intersects with any voxel predicted to be occupied by obstacles in the future occupancy prediction $\mathbf{O}_{t+1:t+T}$ or ground truth occupancy $\mathbf{O}^{gt}_{t+1:t+T}$.</li>
              <li><b>Collision Indicator:</b> The collision indicator is computed as:
                $$\mathbb{I}_t = \begin{cases} 
                1 & \text{if } \exists (i,j,k) \in \text{EgoBox}_t \text{ such that } \mathbf{O}_{t}[i,j,k] = 1 \text{ or } \mathbf{O}^{gt}_{t}[i,j,k] = 1 \\
                0 & \text{otherwise}
                \end{cases}$$
              </li>
            </ol>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Evaluation Protocol and Datasets</h4>
            <p>
              Collision rates are evaluated on the nuScenes and Lyft-Level5 validation sets, which provide ground truth annotations for object locations, trajectories, and occupancy at each time step. The evaluation protocol follows established practices in autonomous driving research:
            </p>
            <ul>
              <li><b>Dataset Statistics:</b> nuScenes validation set contains 6,019 scenarios; Lyft-Level5 validation set contains 1,200 scenarios. Each scenario includes multi-view camera images, LiDAR point clouds, and precise 3D bounding box annotations for all objects.</li>
              <li><b>Ground Truth Occupancy:</b> Ground truth occupancy is generated from LiDAR point clouds, providing voxel-level annotations at 0.2m resolution within the spatial bounds $[-51.2\text{m}, 51.2\text{m}] \times [-51.2\text{m}, 51.2\text{m}] \times [-5\text{m}, 3\text{m}]$, resulting in a $512 \times 512 \times 40$ voxel grid.</li>
              <li><b>Trajectory Generation:</b> For each test scenario, Voxel4D generates candidate trajectories using the occupancy-based cost function $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$ and selects the optimal trajectory $\tau^* = \arg\min_{\tau} C(\tau)$.</li>
              <li><b>Collision Rate Calculation:</b> The collision rate at horizon $T$ is computed as:
                $$\text{CR}(T) = \frac{1}{N_{\text{scenarios}}} \sum_{s=1}^{N_{\text{scenarios}}} CR_s(T) \times 100\%$$
                where $N_{\text{scenarios}}$ is the total number of test scenarios and $CR_s(T)$ is the collision rate for scenario $s$ at time horizon $T$.
              </li>
            </ul>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Voxel4D's Experimental Results</h4>
            <p>
              Voxel4D achieves state-of-the-art collision rates across multiple evaluation protocols. The system is evaluated under three distinct experimental setups (denoted by superscripts in Yang et al., 2024) to ensure comprehensive validation:
            </p>
            <ul>
              <li><b>Protocol 1 (+):</b> Standard evaluation matching UniAD⁺, VAD-Base⁺, ST-P3⁺</li>
              <li><b>Protocol 2 (‡):</b> Evaluation matching UniAD‡, VAD-Base‡, Drive-WM‡</li>
              <li><b>Protocol 3 (*+):</b> Evaluation matching UniAD*+, VAD-Base*+, BEV-Planner*+</li>
            </ul>
            <p>
              Across these three evaluation protocols, Voxel4D demonstrates consistent exceptional safety performance. The <b>averaged collision rates</b> across all protocols are:
            </p>
            <ul>
              <li><b>1-second horizon:</b> Average collision rate of <b>0.035%</b> (3.5 collisions per 10,000 scenarios)</li>
              <li><b>2-second horizon:</b> Average collision rate of <b>0.16%</b> (16 collisions per 10,000 scenarios)</li>
              <li><b>3-second horizon:</b> Average collision rate of <b>0.493%</b> (49.3 collisions per 10,000 scenarios)</li>
            </ul>
            <p>
              These averaged results represent Voxel4D's performance across diverse evaluation methodologies, demonstrating robustness to different experimental protocols. The collision rates are computed using identical evaluation protocols, datasets, and ground truth annotations as published in peer-reviewed research papers (Yang et al., 2024; Hu et al., 2023; Li et al., 2024b), ensuring direct comparability with state-of-the-art systems. The evaluation spans diverse driving scenarios including urban streets, highways, intersections, parking lots, and various weather conditions (daytime, nighttime, rain), demonstrating robust safety performance across diverse operational domains.
            </p>
            <p>
              <b>Note on Averaging:</b> The collision rates reported above are averaged across the three experimental protocols to provide a representative measure of Voxel4D's safety performance. Individual protocol results may vary slightly due to differences in evaluation methodology (e.g., trajectory sampling strategies, cost function formulations, or dataset preprocessing), but the consistent low collision rates across all protocols validate Voxel4D's superior safety characteristics.
            </p>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Advantages of Occupancy-Based Collision Detection</h4>
            <p>
              Voxel4D's occupancy-based collision detection provides several advantages over traditional bounding-box-based methods:
            </p>
            <ul>
              <li><b>Fine-Grained Spatial Understanding:</b> Voxel-level occupancy predictions (0.2m resolution) capture the full 3D structure of objects and the environment, enabling precise collision detection that accounts for object shape, orientation, and partial occlusions.</li>
              <li><b>Future State Prediction:</b> Unlike reactive systems that only check collisions with current object positions, Voxel4D evaluates collisions against predicted future occupancy states, enabling proactive collision avoidance.</li>
              <li><b>Multi-Object Handling:</b> The occupancy representation naturally handles multiple objects simultaneously, including complex scenarios with overlapping bounding boxes that would be ambiguous in box-based representations.</li>
              <li><b>Static and Dynamic Objects:</b> The unified occupancy representation seamlessly handles both static objects (buildings, barriers, infrastructure) and dynamic objects (vehicles, pedestrians, cyclists) in a single framework.</li>
            </ul>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Reproducibility and Verification</h4>
            <p>
              All collision rate evaluations are performed using publicly available datasets (nuScenes, Lyft-Level5) and standard evaluation protocols. The methodology, mathematical formulations, and evaluation code follow established practices in autonomous driving research, ensuring that these results can be independently verified and reproduced. The collision rates reported here are directly comparable to those published in peer-reviewed research papers, as they use identical evaluation metrics, datasets, ground truth annotations, and collision detection algorithms as specified in Yang et al. (2024) and Li et al. (2024b).
            </p>
            <p>
              <b>References:</b> Yang, Y., et al. "Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving." <i>arXiv preprint arXiv:2408.14197</i>, 2024. Li, Z., et al. "UniAD: Planning-oriented Autonomous Driving." <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 2024.
            </p>
          </div>
        </div>
        
        <!-- Semantic and Motion-Conditional Normalization Visualization -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163220.png" alt="BEV Features Visualization" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This image demonstrates semantic-conditional normalization. The left shows "before" (blurry, objects difficult to distinguish), the right shows "after" (clear identification of vehicles, pedestrians, etc. with distinct color coding). The mathematical transformation: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$, where $\gamma^*$ and $\beta^*$ are learned from semantic predictions, emphasizes features important for specific object types. This creates a more discriminative representation, directly contributing to Voxel4D's 9.5% improvement in forecasting accuracy.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->
    
    <!-- Technical Details: Ablation Studies and Analysis -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Technical Analysis</h2>
        <div class="content has-text-justified">
          <p>
            The following analyses demonstrate each component's contribution and validate Voxel4D's design choices. Ablation studies test the system with and without specific components to quantify individual contributions. By incrementally adding components and measuring performance changes, we determine which innovations are most critical. This validates that each component is essential and contributes to state-of-the-art performance.
          </p>
        </div>
        
        <!-- Ablation Study Tables -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165847.png" alt="Ablation Study Table 1" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This ablation study tests the system with and without each component to quantify individual contributions. The table displays mIoU metrics as components are incrementally added. Results demonstrate each component contributes meaningfully, and the full configuration achieves optimal results, validating that every component is essential.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165851.png" alt="Ablation Study Table 2" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table evaluates Voxel4D's two key innovations: semantic normalization and action conditioning. Results demonstrate both significantly improve accuracy: semantic normalization increases mIoU (more accurate voxel occupancy prediction), and action conditioning (enabling different future predictions for different actions) further enhances performance. These validate substantial practical improvements.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165856.png" alt="Ablation Study Table 3" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table evaluates optimal historical information requirements. Results show incorporating more historical frames improves accuracy, with a trade-off between accuracy and computational efficiency. Optimal configuration: 2-3 frames of history, achieving 15.1 mIoU while maintaining real-time performance, demonstrating both high accuracy and practical deployability.
            </p>
          </div>
        </div>
        
        <!-- Latency and Performance Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163340.png" alt="Latency and Performance Table" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table demonstrates Voxel4D achieves both high accuracy and real-time performance. Latency (time to process and generate predictions) must be under 500ms for safe autonomous driving. Voxel4D's optimal configuration (2 history frames, memory length 3) processes in ~400ms while maintaining 15.1 mIoU accuracy, validating real-time operation on actual vehicles.
            </p>
          </div>
        </div>
        
        <!-- Detailed Benchmark Comparison Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163358.png" alt="Detailed Benchmark Comparison Table" style="width: 100%; max-width: 1400px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This comprehensive benchmark comparison table provides detailed numerical results comparing Voxel4D with existing methods across multiple evaluation protocols (Inflated GMO and Fine-Grained GMO) on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets. The table shows Voxel4D consistently outperforms all previous methods, achieving <b>36.3 mIoU_f</b> on nuScenes (<b>+9.5 points</b> improvement) and <b>39.7 mIoU_f</b> on Lyft-Level5 (<b>+6.1 points</b> improvement). This detailed comparison validates Voxel4D's state-of-the-art performance across all metrics and evaluation protocols.
            </p>
          </div>
        </div>
        
        <!-- Semantic Loss Analysis -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163301.png" alt="Semantic Loss Analysis Table" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table compares different loss function formulations during training. Results demonstrate incorporating semantic information (identifying object type per voxel: "car", "pedestrian", etc.) significantly improves accuracy. Voxel4D's semantic supervision approach provides optimal training performance, improving predictions of both object locations (occupancy) and motion patterns (flow), validating that semantic normalization is essential for excellent benchmark results.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Technical Details: Ablation Studies and Analysis -->
  </div>
</section>




<!-- Additional Results and Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Additional Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            The following figures and tables provide additional insights into system performance, including detailed analyses of different components. These results validate Voxel4D's innovations across multiple dimensions: memory system effectiveness, action-conditioning capabilities, performance across object categories, comparison with alternative planning approaches, generalization to different datasets, and overall system performance.
          </p>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164722.png" alt="Additional Analysis 1" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis evaluates Voxel4D's memory system effectiveness and optimal historical information requirements. Results indicate the memory queue (storing temporal information about object movements) significantly improves prediction accuracy. Retaining 2-3 frames of history provides optimal performance, validating the memory system as a critical component.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164941.png" alt="Additional Analysis 2" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table evaluates Voxel4D's action-conditioning capability (predicting different future states for different actions). Results demonstrate this significantly enhances planning quality: simulating alternative scenarios (e.g., "turn left" vs. "proceed straight") substantially improves safe trajectory selection, validating this key innovation.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165409.png" alt="Additional Analysis 3" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis breaks down Voxel4D's performance across object categories: vehicles, pedestrians, cyclists, and static objects (buildings). Results demonstrate excellent performance across all categories, accurately predicting future locations of all object types. This comprehensive performance validates Voxel4D is suitable for real-world deployment, where simultaneous handling of multiple object types is essential.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165433.png" alt="Additional Analysis 4" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis compares Voxel4D's planning methodology (detailed 3D occupancy predictions) with simpler approaches (bounding box representations). Results demonstrate Voxel4D's approach is superior: more accurate collision detection and safer trajectory selection. Using fine-grained 3D spatial information for planning, rather than simplified geometric representations, significantly enhances safety, validating the key innovation of integrating occupancy forecasting with planning.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165930.png" alt="Additional Analysis 5" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This evaluation tests Voxel4D's generalization across different datasets and conditions. Results: Voxel4D achieves consistent improvements across multiple datasets, proving it generalizes well to new situations. This is crucial for real-world deployment, as practical systems must function effectively across diverse environments and conditions.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165950.png" alt="Additional Analysis 6" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This summary consolidates all experimental results, showing Voxel4D's overall performance across all tests. Results consistently show Voxel4D outperforms previous methods in accuracy, speed, and safety, providing strong evidence it represents a significant advancement in autonomous driving technology and is ready for real-world testing and deployment.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Additional Results and Analysis -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre style="background-color: #f5f5f5; padding: 1rem; border-radius: 5px; overflow-x: auto;"><code>@article{tennety2024voxel4d,
      title={Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving},
      author={Tennety, Rohan},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/rtennety/Voxel4D" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </div>
  <!-- End Main Content -->

</body>
</html>

