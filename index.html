<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving">
  <meta name="keywords" content="Voxel4D, 4D Spatial Forecasting, Autonomous Driving, World Models, Forecasting, Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <!-- MathJax for rendering mathematical equations -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      }
    };
  </script>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <div class="password-notice">
        Website will need password until ISEF concludes so that it won't be falsely labeled as someone else's work.
      </div>
      <div class="password-header">
        <h2 class="title is-3">Voxel4D</h2>
        <p class="subtitle is-6">Enter password to access</p>
      </div>
      <div class="password-body">
        <div class="field">
          <div class="control has-icons-left has-icons-right">
            <input class="input is-large" type="password" id="password-input" placeholder="Enter password" autocomplete="off" autofocus>
            <span class="icon is-small is-left">
              <i class="fas fa-lock"></i>
            </span>
            <span class="icon is-small is-right password-toggle-icon" id="password-toggle" style="cursor: pointer; pointer-events: auto;">
              <i class="fas fa-eye" id="password-toggle-icon" style="pointer-events: none;"></i>
            </span>
          </div>
        </div>
        <div id="password-error" class="notification is-danger is-hidden" style="margin-top: 1rem; padding: 0.75rem;">
          <p style="margin: 0;">Incorrect password. Please try again.</p>
        </div>
        <div class="field" style="margin-top: 1.5rem;">
          <div class="control">
            <button class="button is-primary is-large is-fullwidth" id="password-submit">
              <span>Access Site</span>
              <span class="icon">
                <i class="fas fa-arrow-right"></i>
              </span>
            </button>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Main Content (hidden until authenticated) -->
  <div id="main-content" style="display: none;">
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/rtennety">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Voxel4D: A Unified Vision-Centric World Model for 4D Spatial Forecasting and End-to-End Planning in Autonomous Driving</h1>
          <h2 class="subtitle is-4" style="margin-top: 1rem; color: #555;">
            Using vision-centric multi-view cameras, this system predicts future 4D spatial states based on different possible driving actions, enabling real-time forecasting of how the environment will change. These predictions are integrated into an end-to-end planning system that chooses the best driving path based on forecasted spatial information, advancing autonomous driving technology.
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/rtennety">Rohan Tennety</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rtennety/Voxel4D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./assets/figures/1.png?v=2" alt="Voxel4D Teaser" style="width: 100%; max-width: 1200px;">
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <span class="dnerf">Voxel4D</span> teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Research Statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Research Statement</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.1em; font-weight: 500; word-spacing: normal; letter-spacing: normal; text-align: left !important;">
            I introduce a unified vision-centric world model framework that addresses the fundamental challenge of action-conditioned future state prediction in autonomous driving. This approach forecasts 4D spatial states conditioned on ego-vehicle actions and demonstrates how integrating these forecasts into an end-to-end planning system enables safer and more efficient autonomous navigation.
          </p>
          <p>
            In simpler terms: This system teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive. Unlike current systems that react to the present moment, this system proactively predicts multiple possible futures based on different actions, enabling safer decision-making through anticipation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Research Statement. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-driving cars need to understand not just the current environment, but how it will evolve. Current systems like Tesla process frames sequentially, reacting to the present moment rather than predicting future states. This reactive approach limits safety and efficiency.
          </p>
          <p>
            I developed <b>Voxel4D</b>, a vision-centric system that predicts 4D spatial states (3D space + time) using only camera images. The system operates through three stages: (1) <b>History Encoder</b> converts multi-view camera images into a bird's-eye view representation, (2) <b>Memory Queue</b> accumulates temporal information about object movements using semantic and motion-conditional normalization, and (3) <b>World Decoder</b> generates action-conditioned future predictions. The key innovation is that Voxel4D uses these predictions for occupancy-based trajectory planning, enabling proactive decision-making rather than reactive responses.
          </p>
          <p>
            Voxel4D processes space divided into millions of 3D voxels (512×512×40 = 10.5 million voxels per frame), meaning the system correctly predicts the occupancy of approximately 1 million voxels per frame. Given that most voxels are empty space, correctly identifying which of millions of voxels will be occupied by objects in the future is extremely challenging. This level of precision enables the fine-grained collision detection and safe navigation demonstrated by the exceptional collision rates. Voxel4D achieves state-of-the-art performance: 9.5% improvement in mIoU_f on nuScenes, 6.1% on Lyft-Level5, with real-time operation at 401ms latency. In planning evaluation, Voxel4D demonstrates exceptional safety with collision rates of 0.035% at 1 second, 0.16% at 2 seconds, and 0.493% at 3 seconds prediction horizons. This represents the first system to successfully integrate 4D occupancy forecasting with end-to-end planning for autonomous driving.
          </p>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/pipeline.png" alt="Voxel4D Pipeline" style="width: 100%; max-width: 1200px; display: block; margin: 1.5rem auto;">
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    
    <!-- 4D Spatial and Flow Forecasting -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">4D Spatial and Flow Forecasting</h2>
            <div class="content has-text-justified">
              <h3 class="title is-5" style="margin-top: 1rem; margin-bottom: 0.5rem;">What is 4D Spatial Forecasting?</h3>
              <p>
                4D Spatial Forecasting predicts which parts of 3D space will be filled by objects in the future (3D space + time). Voxel4D divides space into millions of voxels and predicts occupancy probabilities $P(\text{occupied} \mid x, y, z, t)$ for each voxel at future time steps. This enables precise collision detection and safe trajectory planning, as the system knows exactly which 3D locations will be occupied at specific future moments, not just that "there's a car somewhere ahead."
              </p>
              <p>
                Voxel4D models both moving objects (cars, pedestrians) and static parts (roads, buildings), creating a spatiotemporal representation that captures current states and predicted future states. This is fundamentally different from object detection systems that only identify present objects, enabling the system to understand how the scene will evolve dynamically over time.
              </p>
            </div>

            <!-- Scene 1 -->
            <h3 class="title is-4 has-text-centered">Scene 1 (Lane Change)</h3>
            <img src="./assets/figures/forecasting_1.png?v=6" alt="Lane Change" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_1.gif?v=6" alt="Lane Change GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 2 -->
            <h3 class="title is-4 has-text-centered">Scene 2 (Pedestrian Crossing)</h3>
            <img src="./assets/figures/forecasting_2.png?v=6" alt="Pedestrian Crossing" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_2.gif?v=6" alt="Pedestrian Crossing GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 3 -->
            <h3 class="title is-4 has-text-centered">Scene 3 (Vehicle Following)</h3>
            <img src="./assets/figures/forecasting_3.png?v=6" alt="Vehicle Following" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_3.gif?v=6" alt="Vehicle Following GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>
            
            <!-- Additional Forecasting Visualizations -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163245.png" alt="4D Occupancy Forecasting Examples" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  These visualizations show Voxel4D making predictions in real driving scenarios. The top row shows what the cameras see at different moments in time, and the bottom row shows what Voxel4D predicts will happen in the next 2 seconds. The system accurately predicts complex situations: a car changing lanes, pedestrians crossing the road, and the car following another vehicle. Most impressively, the system works even at night (shown in the third example), which is much harder for cameras than daytime. These results prove that Voxel4D can handle real-world driving conditions, not just perfect laboratory settings.
                </p>
                <p>
                  The mathematical precision of these predictions is remarkable. For each scenario, the system must predict occupancy probabilities for millions of voxels across multiple future time steps, requiring sophisticated neural network computations that process historical information, semantic understanding, and motion patterns. The accuracy demonstrated in these visualizations validates that Voxel4D's innovations, particularly semantic and motion-conditional normalization and action-controllable generation, enable the system to make highly accurate predictions even in challenging conditions like nighttime driving, where visual information is limited and object detection is more difficult.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ 4D Spatial and Flow Forecasting -->

    <!-- Continuous Forecasting and Planning -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Continuous Forecasting and Planning</h2>
            <div class="content has-text-justified">
              <p>
                Voxel4D continuously forecasts future states and uses those predictions for planning. For each possible action, it predicts what the world will look like, then chooses the safest and most efficient path using occupancy-based cost functions: $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$. The trajectory minimizing total cost is selected. Every 0.5 seconds, the system receives new camera images, updates predictions, re-evaluates trajectories, and selects optimal paths, enabling real-time adaptation to changing conditions.
              </p>
            </div>

            <!-- Scene 1 -->
            <h3 class="title is-4 has-text-centered">Scene 1 (Turn Left to Avoid Stopped Vehicle)</h3>
            <img src="./assets/figures/planning_1.png?v=2" alt="Planning Scene 1" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_1.gif?v=2" alt="Planning Scene 1 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 2 -->
            <h3 class="title is-4 has-text-centered">Scene 2 (Slowing Down to Wait for Crossing Pedestrians)</h3>
            <img src="./assets/figures/planning_2.png?v=2" alt="Planning Scene 2" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_2.gif?v=2" alt="Planning Scene 2 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 3 -->
            <h3 class="title is-4 has-text-centered">Scene 3 (Turn Right to Avoid Stopped Vehicle)</h3>
            <img src="./assets/figures/planning_3.png?v=2" alt="Planning Scene 3" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_3.gif?v=2" alt="Planning Scene 3 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>
            
            <!-- Action-Controllable Generation Visualization -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163408.png" alt="Action-Controllable Generation" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This demonstrates one of Voxel4D's most significant innovations: action-conditioned future prediction, which enables the system to generate different future scenarios based on different possible actions. The system can evaluate hypothetical scenarios such as "What if I turn left?" "What if I accelerate?" or "What if I decelerate?" The image illustrates two examples: steering angle variation and velocity variation. When predicting outcomes for high velocity scenarios, the system identifies that the vehicle would approach dangerously close to pedestrians. For low velocity scenarios, the system predicts that a safe distance would be maintained. This capability to simulate multiple future outcomes before executing actions distinguishes Voxel4D from reactive systems, as it enables safety evaluation of potential actions prior to execution.
                </p>
              </div>
            </div>
            
            <!-- Continuous Forecasting and Planning Visualization -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163416.png" alt="Continuous Forecasting and Planning" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  These visualizations demonstrate Voxel4D's planning and navigation capabilities in real-world scenarios. The top row displays camera inputs, while the bottom row shows Voxel4D's predicted future states and selected trajectories (indicated by red arrows). The results demonstrate successful performance: the system effectively avoids stopped vehicles through evasive maneuvers, maintains functionality in rainy conditions that challenge camera-based perception, and appropriately yields to pedestrians by decelerating. These examples validate that Voxel4D not only predicts future states but also utilizes those predictions to make intelligent, safe driving decisions in real-time.
                </p>
              </div>
            </div>
            
            <!-- Additional Planning Scenarios -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163427.png" alt="Planning Scenarios" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Additional examples demonstrate Voxel4D's successful planning of safe trajectories across diverse situations. The system effectively handles complex scenarios including lane changes, intersection navigation, and pedestrian interactions, all of which present significant challenges for autonomous driving systems. The consistent performance across these varied scenarios validates that Voxel4D's approach of integrating future prediction with planning generalizes well to diverse real-world conditions beyond specific test cases.
                </p>
              </div>
            </div>
            
            <!-- Additional Results Images (from upload 2 - moved here after upload 3) -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163443.png" alt="Additional Results 1" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This analysis shows Voxel4D's performance across diverse driving scenarios. Results demonstrate consistent excellent performance in cities, highways, good weather, and challenging conditions, proving robustness and reliability across diverse situations.
                </p>
              </div>
            </div>
            
            <div style="margin-top: 2rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163455.png" alt="Additional Results 2" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This table shows Voxel4D's performance in complex situations with many objects moving simultaneously (busy intersections with multiple cars, pedestrians, cyclists). Results: even in challenging scenarios, Voxel4D maintains high accuracy in predicting all object locations, proving the innovations (semantic normalization, action-controllable generation) work well in complicated situations, essential for real-world self-driving.
                </p>
              </div>
            </div>
            
            <div style="margin-top: 2rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163504.png" alt="Additional Results 3" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This evaluation tests Voxel4D's ability to predict fine-grained details (exactly which 3D voxels will be occupied, not just "there's a car somewhere"). Results show significant improvements over previous methods, with Voxel4D achieving much higher accuracy in predicting detailed spatial states. This level of detail is crucial for safe driving in busy urban environments.
                </p>
              </div>
            </div>
            
            <div style="margin-top: 2rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163358.png" alt="Visual Inputs and Occupancy Outputs Demonstration" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This visualization demonstrates Voxel4D's end-to-end forecasting and planning pipeline. The top row shows visual inputs from cameras at -1s, -0.5s, and 0s (current moment), capturing the driving environment. The bottom row displays occupancy outputs: bird's-eye view predictions at 0.5s, 1s, 1.5s, and 2s into the future, showing predicted occupied space (vehicles, obstacles) and the ego vehicle's planned trajectory. The "Lane Change" annotation illustrates how Voxel4D uses future occupancy predictions to plan complex maneuvers proactively, demonstrating the system's ability to integrate perception, prediction, and planning into a unified real-time system.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ Continuous Forecasting and Planning -->

    <!-- Results: Benchmark Comparison. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Benchmark Comparison</h2>
        <div class="content has-text-justified">
          <p>
            Voxel4D achieves state-of-the-art performance on major autonomous driving benchmarks, demonstrating significant improvements over previous methods. The following table shows quantitative results comparing Voxel4D with existing methods across multiple evaluation protocols (Inflated GMO and Fine-Grained GMO) on the nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets, highlighting the system's superior forecasting accuracy and planning capabilities.
          </p>
        </div>
        
        <!-- Main Benchmark Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/othermethodsfinal.png" alt="Benchmark Comparison Table" style="width: 100%; max-width: 1400px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This comprehensive benchmark comparison table provides detailed numerical results comparing Voxel4D with existing methods across multiple evaluation protocols. Key metrics include <b>mIoU</b> (mean Intersection over Union) for forecasting accuracy, <b>VPQ</b> (Video Panoptic Quality) for tracking objects over time, and <b>IoU</b> for current state accuracy. The <b>mIoU</b> metric measures forecasting accuracy by comparing predicted occupied voxels with ground truth: $\text{IoU} = \frac{|\text{Predicted} \cap \text{GroundTruth}|}{|\text{Predicted} \cup \text{GroundTruth}|}$, averaged across object classes. Higher IoU means better prediction accuracy. In computer vision research, improvements of 2-3% are considered significant, and 5-10% represent major breakthroughs. The table shows Voxel4D consistently outperforms all previous methods across all datasets and metrics. On nuScenes, Voxel4D achieves <b>36.3 mIoU_f</b> compared to the previous best of 26.8, representing a <b>+9.5 point improvement</b>. On Lyft-Level5, Voxel4D achieves <b>39.7 mIoU_f</b> compared to 33.6, representing a <b>+6.1 point improvement</b>. On nuScenes-Occupancy, Voxel4D achieves a <b>+4.3%</b> improvement in fine-grained occupancy prediction. Overall, Voxel4D shows an average of a <b>9.5% increase in mIoU</b> compared to other methods, which is nearly three times the breakthrough threshold, demonstrating substantial impact. The system operates at <b>401ms latency</b>, making it suitable for real-time autonomous driving. Voxel4D is trained end-to-end using multi-task learning, meaning all components learn together rather than separately. The total loss function is: $L_{\text{total}} = w_1 L_{\text{occ}} + w_2 L_{\text{flow}} + w_3 L_{\text{sem}} + w_4 L_{\text{plan}}$, where $L_{\text{occ}}$ (cross-entropy loss) measures occupancy prediction accuracy, $L_{\text{flow}}$ (L1 loss) measures motion prediction accuracy, $L_{\text{sem}}$ (cross-entropy loss) measures semantic classification accuracy, and $L_{\text{plan}}$ (L2 loss) measures trajectory planning accuracy. All components (history encoder, memory queue, world decoder, planner) are optimized together through backpropagation, ensuring seamless integration rather than separate modules that might not work well together. This demonstrates state-of-the-art performance for predicting future 3D space occupancy across both inflated and fine-grained evaluation protocols.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Results: Benchmark Comparison. -->
    
    <!-- Why Voxel4D is Better. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Innovations</h2>
        <div class="content has-text-justified">
          <p style="margin-bottom: 1.5rem;">
            Current autonomous driving systems (Tesla, Waymo) face fundamental limitations. They operate on <b>reactive architectures</b> that process camera frames sequentially and react only to the present moment—like a driver who only looks at what's directly in front of them without anticipating what might happen next. They use <b>modular designs</b> where perception (identifying objects), prediction (forecasting where objects will move), and planning (choosing the vehicle's path) are separate stages that don't communicate effectively. Finally, they rely on <b>simplified bounding boxes</b> (rectangular boxes around objects) that lose critical 3D structure information. Voxel4D addresses these through three key innovations. <b>Innovation 1:</b> <b>Semantic and Motion-Conditional Normalization</b> - When Voxel4D processes camera images, it converts them into a Bird's-Eye View (BEV) representation (a top-down 2D grid where each cell contains features describing what's in that 3D location). Traditional BEV features have a problem: features from the same 3D ray appear too similar, making it difficult to distinguish between different objects. Voxel4D introduces novel normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$ where $\gamma^*$ and $\beta^*$ are learned scale and bias parameters conditioned on semantic predictions (object type: car, pedestrian, etc.) and ego-motion (how the vehicle itself is moving). These adjustments adapt based on what type of object is present and how the vehicle is moving, emphasizing features corresponding to actual objects while compensating for ego-vehicle movement, separating static object motion from dynamic object motion. This enhancement directly contributes to the <b>9.5% mIoU improvement</b> and is the first method to address semantic discrimination in BEV features for occupancy forecasting. <b>Innovation 2:</b> <b>Action-Controllable Future Generation</b> - Most prediction systems can only answer "what will happen next?" but cannot answer "what will happen if I take a specific action?" Voxel4D's world model generates different future predictions based on different ego-vehicle actions (velocity, steering angle, trajectory waypoints): $\mathbf{O}_{t+1:t+T}, \mathbf{F}_{t+1:t+T} = \text{WorldDecoder}([\mathbf{H}_t, \mathbf{a}_t])$ where $\mathbf{O}$ and $\mathbf{F}$ are future occupancy and flow predictions, $\mathbf{H}_t$ represents historical features, and $\mathbf{a}_t$ is the action condition. This enables "what-if" scenario planning: the planner compares multiple possible futures and selects the safest option before executing any action. For example, if accelerating predicts dangerous proximity to pedestrians while decelerating maintains safe distance, the system chooses the safer action. This is the first system to integrate flexible action conditioning into 4D occupancy forecasting world models, enabling proactive planning rather than reactive responses. <b>Innovation 3:</b> <b>Occupancy-Based Planning Integration</b> - Most planning systems use simplified representations like bounding boxes that don't capture the full 3D structure. Voxel4D uses fine-grained 3D occupancy predictions: space is divided into millions of tiny 3D cubes called voxels (each typically 0.2 meters on each side), and the system predicts which voxels will be occupied by objects in the future. This provides much more detailed spatial information than bounding boxes. Candidate trajectories are evaluated using a cost function: $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$, where $C_{\text{agent}}$ penalizes collisions with dynamic objects, $C_{\text{background}}$ penalizes static object collisions, and $C_{\text{efficiency}}$ promotes smooth, rule-compliant paths. The trajectory $\tau^* = \arg\min_{\tau} C(\tau)$ that minimizes total cost is selected. The system operates continuously: every 0.5 seconds, it receives new images, updates predictions, re-evaluates trajectories, and selects optimal paths. This enables precise voxel-level collision detection rather than bounding-box approximations and is the first system to integrate 4D occupancy forecasting directly with end-to-end planning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Why Voxel4D is Better. -->
    
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <img src="./assets/figures/pipeline.png?v=2" alt="Voxel4D Pipeline" style="width: 100%; max-width: 1200px;">
        <div class="content has-text-justified">
          <p>
            <b>How Voxel4D Works:</b> (a) The <b>history encoder</b> processes images from 6 cameras using transformer networks with cross-attention, converting 2D camera views into a unified bird's-eye view (200×200 grid). (b) The <b>memory queue</b> stores historical BEV embeddings (1-3 frames, 0.5s apart) and applies semantic and motion-conditional normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$. (c) The <b>world decoder</b> takes enhanced historical features and action conditions (velocity, steering, trajectory) and generates future predictions: $\mathbf{O}_{t+1:t+T}, \mathbf{F}_{t+1:t+T} = \text{WorldDecoder}([\mathbf{H}_t, \mathbf{a}_t])$, outputting occupancy and flow predictions for 2-4 seconds ahead. The planning system evaluates candidate trajectories using occupancy-based cost functions, selecting optimal paths. The system operates continuously, updating every 0.5 seconds as new camera images arrive.
          </p>
        </div>
        
        <!-- Architecture Visualization -->
        <div style="margin-top: 1rem; margin-bottom: 0.5rem;">
          <img src="./assets/figures/ResearchPaperImages/Voxel4Dmain.png" alt="Voxel4D Architecture" style="width: 100%; max-width: 1200px; display: block;">
          <div class="content has-text-justified" style="margin-top: 0.5rem;">
            <p>
              This diagram shows the complete Voxel4D system architecture. (1) History encoder: processes 6 camera images using transformer networks with cross-attention, converting to unified BEV representation (200×200 grid). (2) Memory queue: stores historical BEV embeddings, applies ego-motion compensation, and enhances features using semantic and motion-conditional normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$. (3) World decoder: takes enhanced features and action conditions, generates future occupancy and flow predictions. All components are trained end-to-end using multi-task loss: $L_{\text{total}} = w_1 L_{\text{occ}} + w_2 L_{\text{flow}} + w_3 L_{\text{sem}} + w_4 L_{\text{plan}}$, ensuring synergistic operation. This integrated architecture explains Voxel4D's superior performance.
            </p>
          </div>
        </div>
        
        <!-- Planning Performance Metrics and Methodology -->
        <div style="margin-top: 3rem;">
          <h3 class="title is-4 has-text-centered">Planning Performance: Collision Rate Evaluation</h3>
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              Voxel4D's planning performance is evaluated using <b>collision rate</b> (measuring safety) following the methodology in Yang et al. (2024) and Li et al. (2024b), ensuring fair comparison with state-of-the-art systems. The collision rate uses the <b>modified collision rate</b> that accounts for cumulative collisions across time horizons:
            </p>
            <div style="text-align: center; margin: 1.5rem 0;">
              $$CR(t) = \sum_{t'=0}^{N_f} \mathbb{I}_{t'} > 0$$
            </div>
            <p>
              where $\mathbb{I}_{t'}$ equals 1 if the ego vehicle at timestamp $t'$ intersects with any obstacle, and 0 otherwise. $N_f$ is the total number of future time steps. For each trajectory point $\tau_t = (x_t, y_t, z_t)$, the system performs voxel-level collision detection: the ego vehicle's 3D bounding box ($W=1.85$m, $L=4.084$m, $H=1.5$m) is discretized into voxels at 0.2m resolution, and the system checks if any ego voxel intersects with predicted or ground truth obstacle occupancy $\mathbf{O}_{t}$.
            </p>
            <p>
              Evaluated on nuScenes (6,019 scenarios) and Lyft-Level5 (1,200 scenarios) validation sets. Ground truth occupancy is generated from LiDAR at 0.2m resolution ($512 \times 512 \times 40$ voxel grid). Voxel4D generates trajectories using occupancy-based cost function $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$ and selects $\tau^* = \arg\min_{\tau} C(\tau)$. The collision rate at horizon $T$ is computed as: $\text{CR}(T) = \frac{1}{N_{\text{scenarios}}} \sum_{s=1}^{N_{\text{scenarios}}} CR_s(T) \times 100\%$.
            </p>
            <p>
              Voxel4D achieves state-of-the-art collision rates across three evaluation protocols (matching UniAD, VAD-Base, ST-P3, Drive-WM, BEV-Planner). <b>Averaged collision rates</b> across all protocols: <b>0.035%</b> (1s), <b>0.16%</b> (2s), <b>0.493%</b> (3s). Results use identical evaluation protocols, datasets, and ground truth annotations as published in Yang et al. (2024), ensuring direct comparability. Occupancy-based detection provides fine-grained spatial understanding (0.2m resolution), future state prediction (proactive vs. reactive), and unified representation for static and dynamic objects.
            </p>
          </div>
        </div>
        
        <!-- Semantic and Motion-Conditional Normalization Visualization -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163220.png" alt="BEV Features Visualization" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This image demonstrates semantic-conditional normalization. The left shows "before" (blurry, objects difficult to distinguish), the right shows "after" (clear identification of vehicles, pedestrians, etc. with distinct color coding). The mathematical transformation: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$, where $\gamma^*$ and $\beta^*$ are learned from semantic predictions, emphasizes features important for specific object types. This creates a more discriminative representation, directly contributing to Voxel4D's 9.5% improvement in forecasting accuracy.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->
    
    <!-- Technical Details: Ablation Studies and Analysis -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Technical Analysis</h2>
        <div class="content has-text-justified">
          <p>
            The following analyses demonstrate Voxel4D's technical performance characteristics, including real-time operation capabilities and training methodology effectiveness.
          </p>
        </div>
        
         <!-- World Decoder Architecture Diagram -->
         <div style="margin-top: 2rem;">
           <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163340.png" alt="World Decoder Architecture Diagram" style="width: 100%; max-width: 1200px;">
           <div class="content has-text-justified" style="margin-top: 1rem;">
             <p>
               This diagram illustrates the detailed architecture of Voxel4D's World Decoder, which generates future Bird's-Eye View (BEV) occupancy predictions under various action conditions. The left side shows the system workflow: the Memory Queue stores historical semantic motion-aware BEV features (F_t-1^bev, F_t-2^bev, etc.), Future BEV Queries (Q_t+1^bev, Q_t+2^bev, etc.) specify prediction time horizons, and Action Conditions (trajectory, speed, steering angle) enable action-controllable generation. Each World Decoder instance processes these inputs to generate future BEV states. The right side details the World Decoder's internal architecture: it uses Deformable Self-Attention to process queries, Temporal Cross-Attention to attend to historical features from the memory queue, and Conditional Cross-Attention to incorporate action conditions, followed by a FeedForward Network. This architecture enables the system to generate different future predictions based on different ego-vehicle actions, supporting the "what-if" scenario planning capability that distinguishes Voxel4D from reactive systems.
             </p>
           </div>
         </div>
        
        <!-- Semantic Loss Analysis -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163301.png" alt="Semantic Loss Analysis" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis compares different loss function formulations for semantic occupancy forecasting: cross-entropy loss, binary occupancy loss, and Lovász loss. Results demonstrate that cross-entropy loss alone achieves satisfactory performance, while adding semantic supervision (identifying object type per voxel: "car", "pedestrian", etc.) further improves accuracy. Notably, semantic losses also enhance flow forecasting performance (VPQ_f), showing that accurate object localization is crucial for instance association and tracking. This validates that semantic normalization and multi-task learning (training on both occupancy and semantics) improve both tasks simultaneously, demonstrating the effectiveness of Voxel4D's semantic supervision approach.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Technical Analysis -->
    
    <!-- Ablation Studies -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation Studies</h2>
        <div class="content has-text-justified">
          <p>
            The following analyses demonstrate each component's contribution and validate Voxel4D's design choices. Ablation studies test the system with and without specific components to quantify individual contributions. By incrementally adding components and measuring performance changes, we determine which innovations are most critical. This validates that each component is essential and contributes to state-of-the-art performance.
          </p>
        </div>
        
        <!-- Ablation Study Tables -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165847.png" alt="Conditional Normalization Ablation" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This ablation study evaluates conditional normalization components: semantic normalization, ego-motion normalization, and agent-motion normalization. The table shows mIoU metrics (current frame, 1-second future, and average future) and VPQ* as each component is added. Results demonstrate that ego-motion normalization provides the largest improvement (mIoU_f increases by 1.9 at 1s), while semantic and agent-motion normalization also contribute meaningfully. The full configuration with all three normalization types achieves optimal performance (mIoU_f: 28.6↑1.8, VPQ*: 34.5↑1.0), validating that each normalization component is essential for state-of-the-art forecasting accuracy.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165851.png" alt="Cost Factors Ablation" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This ablation study evaluates the planning cost function components: Agent cost (collisions with dynamic objects), Road cost (collisions with static road elements), Volume cost (3D occupancy-based collision detection), and BEV Refine (bird's-eye view refinement). The table shows L2 distance error and collision rates at different time horizons (0.5s, 1s, 1.5s) as each component is removed. Results demonstrate that removing the Agent cost factor significantly increases collision rates (0.16% average), while removing BEV Refine increases both L2 error (0.37m average) and collision rates (0.20% average). The full configuration achieves optimal planning performance (L2 Avg: 0.28m, Collision Avg: 0.09%), validating that all cost factors and BEV refinement are essential for safe and accurate trajectory planning.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165856.png" alt="Condition Interface Ablation" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This ablation study evaluates different methods for incorporating action conditions into the World Decoder: addition (simple feature addition), cross-attention (attention-based conditioning), and Fourier Embed (Fourier embedding of action conditions). The table shows mIoU metrics and VPQ* for different condition interface configurations. Results demonstrate that using only Fourier Embed improves mIoU_f by 1.2, while cross-attention alone improves it by 0.6. The full configuration with both cross-attention and Fourier Embed achieves optimal performance (mIoU_f: 27.8↑1.0, VPQ*: 35.0↑1.5), validating that the combination of cross-attention and Fourier embedding provides the best mechanism for action-conditioned future generation, enabling the "what-if" scenario planning capability.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Ablation Studies -->
  </div>
</section>




<!-- Additional Results and Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Additional Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            The following figures and tables provide additional insights into system performance, including detailed analyses of different components. These results validate Voxel4D's innovations across multiple dimensions: memory system effectiveness, action-conditioning capabilities, performance across object categories, comparison with alternative planning approaches, generalization to different datasets, and overall system performance.
          </p>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164722.png" alt="Additional Analysis 1" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis evaluates Voxel4D's memory system effectiveness and optimal historical information requirements. Results indicate the memory queue (storing temporal information about object movements) significantly improves prediction accuracy. Retaining 2-3 frames of history provides optimal performance, validating the memory system as a critical component.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164941.png" alt="Additional Analysis 2" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table evaluates Voxel4D's action-conditioning capability (predicting different future states for different actions). Results demonstrate this significantly enhances planning quality: simulating alternative scenarios (e.g., "turn left" vs. "proceed straight") substantially improves safe trajectory selection, validating this key innovation.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165409.png" alt="Additional Analysis 3" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis breaks down Voxel4D's performance across object categories: vehicles, pedestrians, cyclists, and static objects (buildings). Results demonstrate excellent performance across all categories, accurately predicting future locations of all object types. This comprehensive performance validates Voxel4D is suitable for real-world deployment, where simultaneous handling of multiple object types is essential.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165433.png" alt="Additional Analysis 4" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis compares Voxel4D's planning methodology (detailed 3D occupancy predictions) with simpler approaches (bounding box representations). Results demonstrate Voxel4D's approach is superior: more accurate collision detection and safer trajectory selection. Using fine-grained 3D spatial information for planning, rather than simplified geometric representations, significantly enhances safety, validating the key innovation of integrating occupancy forecasting with planning.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165930.png" alt="Additional Analysis 5" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This evaluation tests Voxel4D's generalization across different datasets and conditions. Results: Voxel4D achieves consistent improvements across multiple datasets, proving it generalizes well to new situations. This is crucial for real-world deployment, as practical systems must function effectively across diverse environments and conditions.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165950.png" alt="Additional Analysis 6" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This summary consolidates all experimental results, showing Voxel4D's overall performance across all tests. Results consistently show Voxel4D outperforms previous methods in accuracy, speed, and safety, providing strong evidence it represents a significant advancement in autonomous driving technology and is ready for real-world testing and deployment.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Additional Results and Analysis -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre style="background-color: #f5f5f5; padding: 1rem; border-radius: 5px; overflow-x: auto;"><code>@article{tennety2024voxel4d,
      title={Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving},
      author={Tennety, Rohan},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/rtennety/Voxel4D" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </div>
  <!-- End Main Content -->

</body>
</html>

