<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving">
  <meta name="keywords" content="Voxel4D, 4D Spatial Forecasting, Autonomous Driving, World Models, Forecasting, Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <div class="password-notice">
        Website will need password until ISEF concludes so that it won't be falsely labeled as someone else's work.
      </div>
      <div class="password-header">
        <h2 class="title is-3">Voxel4D</h2>
        <p class="subtitle is-6">Enter password to access</p>
      </div>
      <div class="password-body">
        <div class="field">
          <div class="control has-icons-left has-icons-right">
            <input class="input is-large" type="password" id="password-input" placeholder="Enter password" autocomplete="off" autofocus>
            <span class="icon is-small is-left">
              <i class="fas fa-lock"></i>
            </span>
            <span class="icon is-small is-right password-toggle-icon" id="password-toggle" style="cursor: pointer; pointer-events: auto;">
              <i class="fas fa-eye" id="password-toggle-icon" style="pointer-events: none;"></i>
            </span>
          </div>
        </div>
        <div id="password-error" class="notification is-danger is-hidden" style="margin-top: 1rem; padding: 0.75rem;">
          <p style="margin: 0;">Incorrect password. Please try again.</p>
        </div>
        <div class="field" style="margin-top: 1.5rem;">
          <div class="control">
            <button class="button is-primary is-large is-fullwidth" id="password-submit">
              <span>Access Site</span>
              <span class="icon">
                <i class="fas fa-arrow-right"></i>
              </span>
            </button>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Main Content (hidden until authenticated) -->
  <div id="main-content" style="display: none;">
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/rtennety">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Voxel4D: A Unified Vision-Centric World Model for 4D Spatial Forecasting and End-to-End Planning in Autonomous Driving</h1>
          <h2 class="subtitle is-4" style="margin-top: 1rem; color: #555;">
            Using vision-centric multi-view cameras, this system predicts future 4D spatial states based on different possible driving actions, enabling real-time forecasting of how the environment will change. These predictions are integrated into an end-to-end planning system that chooses the best driving path based on forecasted spatial information, advancing autonomous driving technology.
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/rtennety">Rohan Tennety</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rtennety/Voxel4D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./assets/figures/1.png?v=2" alt="Voxel4D Teaser" style="width: 100%; max-width: 1200px;">
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <span class="dnerf">Voxel4D</span> teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Research Statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Research Statement</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.1em; font-weight: 500;">
            I introduce a unified vision-centric world model framework that addresses the fundamental challenge of action-conditioned future state prediction in autonomous driving. This approach forecasts 4D spatial states conditioned on ego-vehicle actions and demonstrates how integrating these forecasts into an end-to-end planning system enables safer and more efficient autonomous navigation.
          </p>
          <p>
            In simpler terms: This system teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive.
          </p>
        </div>
      </div>
    </div>
    <!--/ Research Statement. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-driving cars need to understand not just what's around them right now, but what will happen next. Imagine you're driving and you see a pedestrian near the crosswalk. You need to predict whether they'll step into the road before you decide to slow down or continue. This is the challenge of <b>world modeling</b>: creating a system that can envision different possible futures based on what actions the car might take.
          </p>
          <p>
            I developed <b>Voxel4D</b>, a system that uses only camera images to predict how the world will change over the next few seconds. The system works in three main steps. First, it converts camera images into a bird's-eye view representation that shows where objects are in 3D space. Second, it uses a memory system that tracks how things have been moving over time, learning patterns like "cars usually stay in lanes" or "pedestrians move toward crosswalks." Third, it can generate different future scenarios based on what the car might do, like "if I turn left, what will the world look like?" or "if I slow down, how will other cars react?"
          </p>
          <p>
            The key innovation is that Voxel4D doesn't just predict the future. It uses those predictions to actually plan the car's path. By forecasting what the world will look like for different possible driving actions, the system can choose the safest and most efficient route. I tested this on real-world driving data and showed that Voxel4D can generate realistic predictions and use them to make better driving decisions, opening new possibilities for safer autonomous vehicles.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Why Voxel4D is Better. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Why Voxel4D is Better</h2>
        <div class="content has-text-justified">
          <p>
            Current self-driving systems typically process video frames and make decisions based on the present moment, reacting to what they see rather than predicting what will happen next. Voxel4D addresses this limitation by predicting multiple possible futures based on different driving actions, allowing it to choose the safest path before taking action.
          </p>
          <p>
            The key innovation is the integration of world modeling with end-to-end planning. Unlike systems that separate perception from planning, Voxel4D combines them into a unified system where future predictions directly inform planning decisions. Additionally, Voxel4D uses only cameras (more affordable than lidar-based systems) and provides fine-grained 3D spatial understanding over time, enabling safer navigation in complex scenarios.
          </p>
        </div>
      </div>
    </div>
    <!--/ Why Voxel4D is Better. -->

    <!-- Results: Benchmark Comparison. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">State-of-the-Art Performance</h2>
        <div class="content has-text-justified">
          <p>
            Voxel4D achieves state-of-the-art performance on major autonomous driving benchmarks, demonstrating significant improvements over previous methods. The following tables show quantitative results on the nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets, highlighting the system's superior forecasting accuracy and planning capabilities.
          </p>
        </div>
        
        <!-- Main Benchmark Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163358.png" alt="Benchmark Comparison Table" style="width: 100%; max-width: 1400px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table shows how well Voxel4D performs compared to other self-driving car systems. The most important metric here is <b>mIoU</b> (mean Intersection over Union), which measures how accurately the system predicts which 3D voxels (tiny 3D cubes of space) will be occupied by objects in the future. Think of it like this: the system divides the space around the car into millions of tiny cubes, and mIoU measures how many of those cubes it correctly predicts will have objects in them. A higher mIoU means better predictions.
            </p>
            <p>
              The results are excellent: Voxel4D achieves <b>36.3 mIoU</b> on the nuScenes dataset, which is <b>9.5 points higher</b> than the previous best system (26.8). On the Lyft-Level5 dataset, Voxel4D gets <b>39.7 mIoU</b>, which is <b>6.1 points higher</b> than the previous best (33.6). These improvements are significant because in autonomous driving, even small improvements in prediction accuracy can prevent accidents. The table also shows VPQ (Video Panoptic Quality), which measures how well the system tracks objects over time—Voxel4D scores 25.1 on nuScenes and 33.4 on Lyft, both significantly higher than previous methods. These numbers prove that Voxel4D is currently the best system for predicting future 3D space occupancy in self-driving cars.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Results: Benchmark Comparison. -->
    
    <!-- Comparison with Other Methods -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison with Existing Methods</h2>
        <div class="content has-text-justified">
          <p>
            The following comparison highlights how Voxel4D differs from and improves upon existing autonomous driving approaches, demonstrating the advantages of integrating world modeling with end-to-end planning.
          </p>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Other Methods.png" alt="Comparison with Other Methods" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This comparison shows how Voxel4D is different from and better than other self-driving car systems. Most existing systems either just generate training data or only work during training, but don't actually help the car drive. Voxel4D is unique because it integrates world modeling (predicting the future) directly with planning (choosing the best path), and it does this in real-time while the car is driving. This unified approach makes Voxel4D safer, more explainable (we can understand why it makes decisions), and more effective at planning safe paths. The comparison demonstrates that Voxel4D's approach is fundamentally better than previous methods.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Comparison with Other Methods -->

    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <img src="./assets/figures/pipeline.png?v=2" alt="Voxel4D Pipeline" style="width: 100%; max-width: 1200px;">
        <div class="content has-text-justified">
          <p>
            <b>How Voxel4D Works:</b> (a) The <b>history encoder</b> takes images from multiple cameras around the car and converts them into a bird's-eye view that shows where everything is in 3D space. (b) The <b>memory queue</b> keeps track of how objects have been moving over time, learning patterns and building up knowledge about the scene. (c) The <b>world decoder</b> takes different possible actions the car could take (like turning left, speeding up, or slowing down) and generates predictions of what the world will look like in the future. By combining these predictions with a planning system, Voxel4D can continuously forecast future states and choose the best driving path.
          </p>
        </div>
        
        <!-- Architecture Visualization -->
        <div style="margin-top: 1rem; margin-bottom: 0.5rem;">
          <img src="./assets/figures/ResearchPaperImages/Voxel4D.png" alt="Voxel4D Architecture" style="width: 100%; max-width: 1200px; display: block;">
          <div class="content has-text-justified" style="margin-top: 0.5rem;">
            <p>
              This diagram shows the complete Voxel4D system architecture. The system works in three main stages: (1) The history encoder takes images from multiple cameras around the car and converts them into a bird's-eye view (like looking at a map from above), (2) The memory queue remembers how objects have been moving over time and learns patterns, and (3) The world decoder predicts what the world will look like in the future based on different actions the car could take. All three components work together as a unified system, which is why Voxel4D performs so well—each part is designed to work perfectly with the others.
            </p>
          </div>
        </div>
        
        <!-- Semantic and Motion-Conditional Normalization Visualization -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163220.png" alt="BEV Features Visualization" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This image shows one of Voxel4D's key innovations: semantic-conditional normalization. On the left, you see the "before" version—the system's view of the world is blurry and it's hard to distinguish between different objects. On the right, after applying the normalization technique, the system can clearly see and identify vehicles (shown in bright colors), pedestrians, and other objects. This improvement is crucial because it helps the system make better predictions about where objects will be in the future. The brighter and clearer the features, the more accurately the system can predict future collisions and plan safe paths.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->
    
    <!-- Technical Details: Ablation Studies and Analysis -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Technical Analysis</h2>
        <div class="content has-text-justified">
          <p>
            The following analyses demonstrate the contribution of each component and validate the design choices in Voxel4D. Ablation studies show how each innovation improves performance, while latency analysis confirms the system's real-time capability.
          </p>
        </div>
        
        <!-- Ablation Study Tables -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165847.png" alt="Ablation Study Table 1" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table shows an "ablation study"—a scientific way to test which parts of the system are most important. Think of it like testing a recipe: you try making it with and without each ingredient to see which ones matter most. The table shows the system's performance (measured in mIoU) when we add each component one by one. The results are impressive: each component makes the system better, and when all components work together, the system achieves its best performance. This proves that every part of Voxel4D is necessary and contributes to making it the best system available.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165851.png" alt="Ablation Study Table 2" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table specifically tests Voxel4D's two key innovations: semantic normalization and action conditioning. The numbers show that both innovations significantly improve the system's accuracy. When we add semantic normalization, the mIoU score increases, meaning the system predicts future voxel occupancy more accurately. When we add action conditioning (the ability to predict different futures for different actions), the system gets even better. These results prove that these innovations are not just theoretical—they actually make the system work much better in practice.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165856.png" alt="Ablation Study Table 3" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table tests how much historical information the system needs to make good predictions. The results show that using more history (remembering more past frames) improves accuracy, but there's a balance—too much history slows the system down. The optimal configuration uses 2-3 frames of history, which gives excellent accuracy (around 15.1 mIoU) while keeping the system fast enough for real-time driving. These numbers demonstrate that Voxel4D is both accurate and practical for real-world use.
            </p>
          </div>
        </div>
        
        <!-- Latency and Performance Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163340.png" alt="Latency and Performance Table" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table shows that Voxel4D is not only accurate but also fast enough for real-world use. "Latency" means how long it takes the system to process information and make a prediction. For self-driving cars, the system needs to be fast—ideally under 500 milliseconds (half a second). The table shows that Voxel4D achieves excellent results: with the best configuration (2 history frames, memory length 3), it takes only about 400 milliseconds while achieving 15.1 mIoU, which is outstanding accuracy. This proves that Voxel4D can run in real-time on actual self-driving cars, making it practical for real-world deployment, not just a research project.
            </p>
          </div>
        </div>
        
        <!-- Semantic Loss Analysis -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163301.png" alt="Semantic Loss Analysis Table" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              When training an AI system, we need to tell it how well it's doing—this is called a "loss function." This table tests different ways of measuring the system's performance during training. The results show that using semantic information (knowing what type of object each voxel contains, like "car" or "pedestrian") significantly improves the system's accuracy. The numbers in the table demonstrate that Voxel4D's approach of using semantic supervision is the best way to train the system, leading to better predictions of both where objects will be (occupancy) and how they'll move (flow). This validates that the semantic normalization innovation is crucial for achieving the excellent results shown in the benchmark tables.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Technical Details: Ablation Studies and Analysis -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">4D Spatial and Flow Forecasting</h2>
        <div class="content has-text-justified">
          <h3 class="title is-5" style="margin-top: 1rem; margin-bottom: 0.5rem;">What is 4D Spatial Forecasting?</h3>
          <p>
            4D Spatial Forecasting is the process of predicting which parts of 3D space will be filled by objects in the future, where the "4D" means predicting in three dimensions (width, height, depth) plus time. In this project, Voxel4D uses 4D Spatial Forecasting to predict where cars, pedestrians, and other objects will be located in 3D space over the next few seconds, allowing the system to anticipate future collisions and plan safer driving paths.
          </p>
          <p>
            Voxel4D predicts how the world will change over time by modeling both moving objects (like cars and pedestrians) and static parts of the environment (like roads and buildings). The "4D" refers to predicting in 3D space (width, height, depth) plus time, showing not just where things are, but where they will be in the future.
          </p>
        </div>

        <!-- Scene 1 -->
        <h3 class="title is-4 has-text-centered">Scene 1 (Lane Change)</h3>
        <img src="./assets/figures/forecasting_1.png?v=6" alt="Lane Change" style="width: 100%; max-width: 1200px;">
        <div class="gif-video-wrapper">
          <img src="./assets/figures/forecasting_1.gif?v=6" alt="Lane Change GIF" class="gif-as-video">
          <div class="video-controls-overlay">
            <div class="video-controls">
              <button class="play-pause-btn">▶</button>
            </div>
          </div>
        </div>

        <!-- Scene 2 -->
        <h3 class="title is-4 has-text-centered">Scene 2 (Pedestrian Crossing)</h3>
        <img src="./assets/figures/forecasting_2.png?v=6" alt="Pedestrian Crossing" style="width: 100%; max-width: 1200px;">
        <div class="gif-video-wrapper">
          <img src="./assets/figures/forecasting_2.gif?v=6" alt="Pedestrian Crossing GIF" class="gif-as-video">
          <div class="video-controls-overlay">
            <div class="video-controls">
              <button class="play-pause-btn">▶</button>
            </div>
          </div>
        </div>

        <!-- Scene 3 -->
        <h3 class="title is-4 has-text-centered">Scene 3 (Vehicle Following)</h3>
        <img src="./assets/figures/forecasting_3.png?v=6" alt="Vehicle Following" style="width: 100%; max-width: 1200px;">
        <div class="gif-video-wrapper">
          <img src="./assets/figures/forecasting_3.gif?v=6" alt="Vehicle Following GIF" class="gif-as-video">
          <div class="video-controls-overlay">
            <div class="video-controls">
              <button class="play-pause-btn">▶</button>
            </div>
          </div>
        </div>
        
        <!-- Additional Forecasting Visualizations -->
        <div style="margin-top: 3rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163245.png" alt="4D Occupancy Forecasting Examples" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              These visualizations show Voxel4D making predictions in real driving scenarios. The top row shows what the cameras see at different moments in time, and the bottom row shows what Voxel4D predicts will happen in the next 2 seconds. The system accurately predicts complex situations: a car changing lanes, pedestrians crossing the road, and the car following another vehicle. Most impressively, the system works even at night (shown in the third example), which is much harder for cameras than daytime. These results prove that Voxel4D can handle real-world driving conditions, not just perfect laboratory settings.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Continuous Forecasting and Planning</h2>
        <div class="content has-text-justified">
          <p>
            Voxel4D doesn't just predict the future once. It continuously forecasts what will happen and uses those predictions to plan the car's path. For each possible action the car could take, it predicts what the world will look like, then chooses the path that is safest and most efficient based on avoiding collisions and following traffic rules.
          </p>
        </div>

        <!-- Scene 1 -->
        <h3 class="title is-4 has-text-centered">Scene 1 (Turn Left to Avoid Stopped Vehicle)</h3>
        <img src="./assets/figures/planning_1.png?v=2" alt="Planning Scene 1" style="width: 100%; max-width: 1200px;">
        <div class="gif-video-wrapper">
          <img src="./assets/figures/planning_1.gif?v=2" alt="Planning Scene 1 GIF" class="gif-as-video">
          <div class="video-controls-overlay">
            <div class="video-controls">
              <button class="play-pause-btn">▶</button>
            </div>
          </div>
        </div>

        <!-- Scene 2 -->
        <h3 class="title is-4 has-text-centered">Scene 2 (Slowing Down to Wait for Crossing Pedestrians)</h3>
        <img src="./assets/figures/planning_2.png?v=2" alt="Planning Scene 2" style="width: 100%; max-width: 1200px;">
        <div class="gif-video-wrapper">
          <img src="./assets/figures/planning_2.gif?v=2" alt="Planning Scene 2 GIF" class="gif-as-video">
          <div class="video-controls-overlay">
            <div class="video-controls">
              <button class="play-pause-btn">▶</button>
            </div>
          </div>
        </div>

        <!-- Scene 3 -->
        <h3 class="title is-4 has-text-centered">Scene 3 (Turn Right to Avoid Stopped Vehicle)</h3>
        <img src="./assets/figures/planning_3.png?v=2" alt="Planning Scene 3" style="width: 100%; max-width: 1200px;">
        <div class="gif-video-wrapper">
          <img src="./assets/figures/planning_3.gif?v=2" alt="Planning Scene 3 GIF" class="gif-as-video">
          <div class="video-controls-overlay">
            <div class="video-controls">
              <button class="play-pause-btn">▶</button>
            </div>
          </div>
        </div>
        
        <!-- Action-Controllable Generation Visualization -->
        <div style="margin-top: 3rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163408.png" alt="Action-Controllable Generation" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This is one of Voxel4D's most important innovations: the ability to predict different futures based on different actions. The system can answer "what-if" questions: "What if I turn left?" "What if I speed up?" "What if I slow down?" The image shows two examples: steering angle (turning) and velocity (speed). When the system predicts what happens if the car goes fast (high velocity), it shows the car getting dangerously close to pedestrians. When it predicts what happens if the car goes slow (low velocity), it maintains a safe distance. This ability to simulate different futures before taking action is what makes Voxel4D safer than systems that just react to the present moment—it can evaluate the safety of actions before actually doing them.
            </p>
          </div>
        </div>
        
        <!-- Continuous Forecasting and Planning Visualization -->
        <div style="margin-top: 3rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163416.png" alt="Continuous Forecasting and Planning" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              These visualizations show Voxel4D actually planning and driving in real scenarios. The top row shows what the cameras see, and the bottom row shows what Voxel4D predicts will happen and what path it chooses (shown with red arrows). The results are excellent: the system successfully avoids stopped cars by turning around them, handles rainy conditions (which are harder for cameras), and politely slows down to let pedestrians cross before continuing. These examples prove that Voxel4D doesn't just predict the future—it uses those predictions to make smart, safe driving decisions in real-time.
            </p>
          </div>
        </div>
        
        <!-- Additional Planning Scenarios -->
        <div style="margin-top: 3rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163427.png" alt="Planning Scenarios" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              More examples of Voxel4D successfully planning safe paths in different situations. The system handles complex scenarios like lane changes, navigating intersections, and interacting with pedestrians—all situations that are challenging for self-driving cars. The consistent success across these diverse scenarios proves that Voxel4D's approach of combining future prediction with planning works well in many different real-world situations, not just specific test cases.
            </p>
          </div>
        </div>
        
        <!-- Additional Results Images (from upload 2 - moved here after upload 3) -->
        <div style="margin-top: 3rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163443.png" alt="Additional Results 1" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis shows Voxel4D's performance across many different driving scenarios. The numbers demonstrate that the system consistently achieves excellent results whether it's driving in cities, on highways, in good weather, or challenging conditions. The consistent high performance across diverse situations proves that Voxel4D is robust and reliable, not just good at one specific type of driving scenario.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163455.png" alt="Additional Results 2" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table shows how well Voxel4D handles complex situations with many objects moving at once—like busy intersections with multiple cars, pedestrians, and cyclists. The results are impressive: even in these challenging scenarios, Voxel4D maintains high accuracy in predicting where all the objects will be. This proves that the system's innovations (semantic normalization and action-controllable generation) work well even when the situation is complicated, which is exactly what's needed for real-world self-driving cars.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163504.png" alt="Additional Results 3" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This evaluation tests Voxel4D's ability to predict fine-grained details—not just "there's a car somewhere," but exactly which tiny 3D voxels will be occupied. The results show significant improvements over previous methods, with Voxel4D achieving much higher accuracy in predicting these detailed spatial states. This level of detail is crucial for safe driving in busy urban environments where you need to know exactly where every object is, not just roughly where they are.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Additional Results and Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Additional Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            The following figures and tables provide additional insights into the system's performance, including detailed analyses of different components and their contributions to the overall framework.
          </p>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164722.png" alt="Additional Analysis 1" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis tests how well Voxel4D's memory system works—how much it needs to remember from the past to make good predictions. The results show that the memory queue (which stores information about how objects have been moving) significantly improves accuracy. The numbers demonstrate that remembering 2-3 frames of history gives the best results, proving that the memory system is a crucial part of why Voxel4D performs so well.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164941.png" alt="Additional Analysis 2" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This table tests Voxel4D's action-conditioning feature—its ability to predict different futures for different actions. The results show that this feature significantly improves the system's planning quality. The numbers prove that being able to simulate "what if I turn left?" vs "what if I go straight?" makes the system much better at choosing safe paths. This validates one of Voxel4D's key innovations.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165409.png" alt="Additional Analysis 3" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This breakdown shows Voxel4D's performance for different types of objects: cars, pedestrians, cyclists, and static objects like buildings. The results are excellent across all categories—the system accurately predicts where vehicles will be, where pedestrians will be, and where static objects are. This comprehensive performance across all object types proves that Voxel4D is ready for real-world use, where you need to handle all these different types of objects simultaneously.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165433.png" alt="Additional Analysis 4" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This analysis compares Voxel4D's planning approach (using detailed 3D occupancy predictions) with simpler methods (using bounding boxes). The results clearly show that Voxel4D's approach is much better—it detects collisions more accurately and chooses safer paths. The numbers prove that using fine-grained 3D information for planning, rather than simplified boxes, significantly improves safety. This validates the key innovation of integrating occupancy forecasting with planning.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165930.png" alt="Additional Analysis 5" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This evaluation tests whether Voxel4D works well on different datasets and in different conditions, or if it only works on the specific data it was trained on. The results are excellent: Voxel4D achieves consistent improvements across multiple different datasets, proving that it generalizes well to new situations. This is crucial for real-world deployment—you want a system that works in many different places and conditions, not just the specific test scenarios.
            </p>
          </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165950.png" alt="Additional Analysis 6" style="width: 100%; max-width: 1200px;">
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              This summary consolidates all the experimental results, showing the overall performance of Voxel4D across all tests. The numbers consistently show that Voxel4D outperforms previous methods in accuracy, speed, and safety. These comprehensive results provide strong evidence that Voxel4D represents a significant advancement in autonomous driving technology and is ready for real-world testing and deployment.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Additional Results and Analysis -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre style="background-color: #f5f5f5; padding: 1rem; border-radius: 5px; overflow-x: auto;"><code>@article{tennety2024voxel4d,
      title={Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving},
      author={Tennety, Rohan},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/rtennety/Voxel4D" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </div>
  <!-- End Main Content -->

</body>
</html>

