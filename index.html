<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Voxel4D: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving">
  <meta name="keywords" content="Voxel4D, 4D Occupancy, Autonomous Driving, World Models, Forecasting, Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Voxel4D: Vision-Centric 4D Occupancy Forecasting and Planning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <div class="password-notice">
        Website will need password until ISEF concludes so that it won't be falsely labeled as someone else's work.
      </div>
      <div class="password-header">
        <h2 class="title is-3">Voxel4D</h2>
        <p class="subtitle is-6">Enter password to access</p>
      </div>
      <div class="password-body">
        <div class="field">
          <div class="control has-icons-left has-icons-right">
            <input class="input is-large" type="password" id="password-input" placeholder="Enter password" autocomplete="off" autofocus>
            <span class="icon is-small is-left">
              <i class="fas fa-lock"></i>
            </span>
            <span class="icon is-small is-right password-toggle-icon" id="password-toggle" style="cursor: pointer; pointer-events: auto;">
              <i class="fas fa-eye" id="password-toggle-icon" style="pointer-events: none;"></i>
            </span>
          </div>
        </div>
        <div id="password-error" class="notification is-danger is-hidden" style="margin-top: 1rem; padding: 0.75rem;">
          <p style="margin: 0;">Incorrect password. Please try again.</p>
        </div>
        <div class="field" style="margin-top: 1.5rem;">
          <div class="control">
            <button class="button is-primary is-large is-fullwidth" id="password-submit">
              <span>Access Site</span>
              <span class="icon">
                <i class="fas fa-arrow-right"></i>
              </span>
            </button>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Main Content (hidden until authenticated) -->
  <div id="main-content" style="display: none;">
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/rtennety">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Voxel4D: A Unified Vision-Centric World Model for 4D Occupancy Forecasting and End-to-End Planning in Autonomous Driving</h1>
          <h2 class="subtitle is-4" style="margin-top: 1rem; color: #ccc;">
            Using vision-centric multi-view cameras, this system predicts future 4D occupancy states based on different possible driving actions, enabling real-time forecasting of how the environment will change. These predictions are integrated into an end-to-end planning system that chooses the best driving path based on forecasted occupancy, advancing autonomous driving technology.
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/rtennety">Rohan Tennety</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rtennety/Voxel4D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="image-protected">
        <img src="./assets/figures/1.png?v=2" alt="Voxel4D Teaser" style="width: 100%; max-width: 1200px;">
        <div class="image-overlay"></div>
      </div>
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <span class="dnerf">Voxel4D</span> teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Research Statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Research Statement</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.1em; font-weight: 500;">
            I introduce a unified vision-centric world model framework that addresses the fundamental challenge of action-conditioned future state prediction in autonomous driving. This approach forecasts 4D occupancy conditioned on ego-vehicle actions and demonstrates how integrating these forecasts into an end-to-end planning system enables safer and more efficient autonomous navigation.
          </p>
          <p>
            In simpler terms: This system teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive.
          </p>
        </div>
      </div>
    </div>
    <!--/ Research Statement. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-driving cars need to understand not just what's around them right now, but what will happen next. Imagine you're driving and you see a pedestrian near the crosswalk. You need to predict whether they'll step into the road before you decide to slow down or continue. This is the challenge of <b>world modeling</b>: creating a system that can envision different possible futures based on what actions the car might take.
          </p>
          <p>
            I developed <b>Voxel4D</b>, a system that uses only camera images to predict how the world will change over the next few seconds. The system works in three main steps. First, it converts camera images into a bird's-eye view representation that shows where objects are in 3D space. Second, it uses a memory system that tracks how things have been moving over time, learning patterns like "cars usually stay in lanes" or "pedestrians move toward crosswalks." Third, it can generate different future scenarios based on what the car might do, like "if I turn left, what will the world look like?" or "if I slow down, how will other cars react?"
          </p>
          <p>
            The key innovation is that Voxel4D doesn't just predict the future. It uses those predictions to actually plan the car's path. By forecasting what the world will look like for different possible driving actions, the system can choose the safest and most efficient route. I tested this on real-world driving data and showed that Voxel4D can generate realistic predictions and use them to make better driving decisions, opening new possibilities for safer autonomous vehicles.
          </p>
          <p>
            What makes Voxel4D special is its ability to understand both the geometry and the motion of everything around the car. Traditional systems might only see where objects are right now, but Voxel4D sees where they're going. It tracks not just cars and pedestrians, but also understands how the entire 3D space around the vehicle will be occupied in the future. This includes predicting both moving objects (like vehicles changing lanes) and static elements (like how the road itself appears from different angles as the car moves).
          </p>
          <p>
            The system is also flexible in how it receives instructions. I can tell it to consider different driving actions, such as specific velocities, steering angles, or even high-level commands like "turn left" or "slow down." This allows the system to explore multiple possible futures before making a decision, similar to how a human driver might think "if I turn here, what could go wrong?" before actually turning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="image-protected">
          <img src="./static/images/pipeline.png?v=2" alt="Voxel4D Pipeline" style="width: 100%; max-width: 1200px;">
          <div class="image-overlay"></div>
        </div>
        <div class="content has-text-justified">
          <p>
            <b>How Voxel4D Works:</b> The system has three main components that work together to predict the future and plan safe driving paths.
          </p>
          <p>
            <b>(a) History Encoder:</b> The history encoder is like the eyes of the system. It takes images from multiple cameras positioned around the car (typically six cameras that see in all directions) and converts them into what's called a "bird's-eye view" representation. Think of it like looking at a map from above, but instead of just showing roads, it shows where every object is in 3D space. This includes cars, pedestrians, bicycles, and even static objects like traffic signs and buildings. The encoder uses advanced computer vision techniques to understand the geometry of the scene, essentially building a 3D model of the world around the car from 2D camera images.
          </p>
          <p>
            <b>(b) Memory Queue:</b> The memory queue is like the system's memory. It doesn't just look at one moment in time, but keeps track of how things have been moving over the past few seconds. This is crucial because understanding motion helps predict future motion. For example, if a car has been moving to the left for the past two seconds, it's likely to continue moving left. The memory queue accumulates this information using a technique called semantic and motion-conditional normalization, which helps the system focus on the most important features. It learns patterns like "cars usually stay in their lanes" or "pedestrians near crosswalks often cross the street." This accumulated knowledge makes the predictions much more accurate.
          </p>
          <p>
            <b>(c) World Decoder:</b> The world decoder is where the magic happens. It takes the information from the history encoder and memory queue, along with different possible actions the car could take (like turning left, speeding up, or slowing down), and generates predictions of what the world will look like in the future. This is called "action-controllable generation" because the predictions change based on what action you're considering. The decoder models both occupancy (where objects will be) and flow (how they'll move), creating a complete picture of future states. By combining these predictions with a planning system that evaluates safety and efficiency, Voxel4D can continuously forecast future states and choose the best driving path.
          </p>
          <p>
            The entire system works in a loop: it observes the world, predicts what will happen for different actions, chooses the best action, executes it, and then repeats. This continuous cycle allows the car to adapt to changing situations in real-time, making it much safer than systems that only look at the current moment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">4D Occupancy and Flow Forecasting</h2>
        <div class="content has-text-justified">
          <p>
            Voxel4D predicts how the world will change over time by modeling both moving objects (like cars and pedestrians) and static parts of the environment (like roads and buildings). The "4D" refers to predicting in 3D space (width, height, depth) plus time, showing not just where things are, but where they will be in the future.
          </p>
          <p>
            <b>Understanding 4D Occupancy:</b> Traditional systems might only detect objects as points or boxes, but Voxel4D understands the full 3D space around the vehicle. It divides the space into small cubes called "voxels" (think of them like 3D pixels) and predicts which voxels will be occupied by objects in the future. This is much more detailed than just knowing "there's a car ahead." The system knows exactly which parts of the 3D space will be filled, including the full shape and size of objects. This level of detail is crucial for safe navigation, especially in complex scenarios like parking lots or intersections where precise spatial understanding matters.
          </p>
          <p>
            <b>Flow Prediction:</b> In addition to predicting where objects will be, Voxel4D also predicts how they'll move. This is called "flow forecasting." For each object, the system predicts its motion vector, which tells you not just where it will be, but the direction and speed it's moving. This is especially important for understanding dynamic scenes. For example, if a pedestrian is walking toward a crosswalk, the flow prediction shows their trajectory, helping the system anticipate when they might enter the road.
          </p>
          <p>
            <b>Why This Matters:</b> By understanding both occupancy and flow, Voxel4D can make much better predictions than systems that only look at current positions. It can anticipate lane changes, predict pedestrian crossings, and understand complex interactions between multiple objects. This comprehensive understanding of future states is what enables safe autonomous driving in real-world scenarios.
          </p>
        </div>

        <!-- Scene 1 -->
        <h3 class="title is-4">Scene 1 (Lane Change)</h3>
        <div class="content has-text-centered">
          <div class="image-protected">
            <img src="./static/images/forecasting_1.gif?v=2" alt="Lane Change" style="width: 100%; max-width: 800px;">
            <div class="image-overlay"></div>
          </div>
        </div>

        <!-- Scene 2 -->
        <h3 class="title is-4">Scene 2 (Pedestrian Crossing)</h3>
        <div class="content has-text-centered">
          <div class="image-protected">
            <img src="./static/images/forecasting_2.gif?v=2" alt="Pedestrian Crossing" style="width: 100%; max-width: 800px;">
            <div class="image-overlay"></div>
          </div>
        </div>

        <!-- Scene 3 -->
        <h3 class="title is-4">Scene 3 (Vehicle Following)</h3>
        <div class="content has-text-centered">
          <div class="image-protected">
            <img src="./static/images/forecasting_3.gif?v=2" alt="Vehicle Following" style="width: 100%; max-width: 800px;">
            <div class="image-overlay"></div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Continuous Forecasting and Planning</h2>
        <div class="content has-text-justified">
          <p>
            Voxel4D doesn't just predict the future once. It continuously forecasts what will happen and uses those predictions to plan the car's path. For each possible action the car could take, it predicts what the world will look like, then chooses the path that is safest and most efficient based on avoiding collisions and following traffic rules.
          </p>
          <p>
            <b>Action-Controllable Generation:</b> One of the key features of Voxel4D is its ability to generate different future scenarios based on different actions. This is called "action-controllable generation." The system can take various inputs to control what kind of future it predicts. For example, I can give it a specific velocity (like "drive at 30 miles per hour") or a steering angle (like "turn 15 degrees to the left"), and it will show me what the world would look like if the car took that action. This flexibility allows the system to explore many possible futures before making a decision, similar to how a human driver might mentally simulate different actions before choosing one.
          </p>
          <p>
            <b>Occupancy-Based Planning:</b> Once the system has predicted multiple possible futures for different actions, it needs to choose the best one. This is where the planning component comes in. The planner uses what's called an "occupancy-based cost function" to evaluate each possible trajectory. This cost function considers several factors: safety (avoiding collisions with predicted future occupancy), efficiency (choosing smooth, fuel-efficient paths), and adherence to traffic rules. By evaluating all possible actions against these criteria, the system selects the optimal path.
          </p>
          <p>
            <b>Continuous Operation:</b> The system doesn't just plan once and execute. Instead, it works in a continuous loop. Every fraction of a second, it observes the current state, predicts futures for different actions, selects the best action, and executes it. Then it immediately starts the process again with the new observations. This continuous operation allows the system to adapt to unexpected changes, like a car suddenly braking ahead or a pedestrian stepping into the road. The predicted future state and selected optimal trajectory are fed back into the world model for the next cycle, creating a seamless integration between perception, prediction, and planning.
          </p>
          <p>
            <b>Real-World Benefits:</b> This approach has significant advantages over traditional planning methods. Because the system understands future occupancy in detail, it can avoid collisions more effectively. It can also handle complex scenarios that would confuse simpler systems, such as navigating through construction zones, handling multiple pedestrians crossing at once, or dealing with vehicles making unexpected lane changes. The continuous nature of the system means it's always ready to adapt, making it much safer for real-world autonomous driving.
          </p>
        </div>

        <!-- Scene 1 -->
        <h3 class="title is-4">Scene 1 (Turn Left to Avoid Stopped Vehicle)</h3>
        <div class="content has-text-centered">
          <div class="image-protected">
            <img src="./static/images/planning_1.png?v=2" alt="Planning Scene 1" style="width: 100%; max-width: 800px;">
            <div class="image-overlay"></div>
          </div>
          <div class="image-protected" style="margin-top: 10px; margin-left: auto; margin-right: auto; display: block; width: fit-content;">
            <img src="./static/images/planning_1.gif?v=2" alt="Planning Scene 1 GIF" style="width: 100%; max-width: 400px;">
            <div class="image-overlay"></div>
          </div>
        </div>

        <!-- Scene 2 -->
        <h3 class="title is-4">Scene 2 (Slowing Down to Wait for Crossing Pedestrians)</h3>
        <div class="content has-text-centered">
          <div class="image-protected">
            <img src="./static/images/planning_2.png?v=2" alt="Planning Scene 2" style="width: 100%; max-width: 800px;">
            <div class="image-overlay"></div>
          </div>
          <div class="image-protected" style="margin-top: 10px; margin-left: auto; margin-right: auto; display: block; width: fit-content;">
            <img src="./static/images/planning_2.gif?v=2" alt="Planning Scene 2 GIF" style="width: 100%; max-width: 400px;">
            <div class="image-overlay"></div>
          </div>
        </div>

        <!-- Scene 3 -->
        <h3 class="title is-4">Scene 3 (Turn Right to Avoid Stopped Vehicle)</h3>
        <div class="content has-text-centered">
          <div class="image-protected">
            <img src="./static/images/planning_3.png?v=2" alt="Planning Scene 3" style="width: 100%; max-width: 800px;">
            <div class="image-overlay"></div>
          </div>
          <div class="image-protected" style="margin-top: 10px; margin-left: auto; margin-right: auto; display: block; width: fit-content;">
            <img src="./static/images/planning_3.gif?v=2" alt="Planning Scene 3 GIF" style="width: 100%; max-width: 400px;">
            <div class="image-overlay"></div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre style="background-color: #1a1a1a; padding: 1rem; border-radius: 5px; overflow-x: auto; color: #ffffff; border: 1px solid #333;"><code style="color: #ffffff;">@article{tennety2024voxel4d,
      title={Voxel4D: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving},
      author={Tennety, Rohan},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/rtennety/Voxel4D" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </div>
  <!-- End Main Content -->

</body>
</html>

