<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving">
  <meta name="keywords" content="Voxel4D, 4D Spatial Forecasting, Autonomous Driving, World Models, Forecasting, Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <!-- MathJax for rendering mathematical equations -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      }
    };
  </script>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <div class="password-notice">
        Website will need password until ISEF concludes so that it won't be falsely labeled as someone else's work.
      </div>
      <div class="password-header">
        <h2 class="title is-3">Voxel4D</h2>
        <p class="subtitle is-6">Enter password to access</p>
      </div>
      <div class="password-body">
        <div class="field">
          <div class="control has-icons-left has-icons-right">
            <input class="input is-large" type="password" id="password-input" placeholder="Enter password" autocomplete="off" autofocus>
            <span class="icon is-small is-left">
              <i class="fas fa-lock"></i>
            </span>
            <span class="icon is-small is-right password-toggle-icon" id="password-toggle" style="cursor: pointer; pointer-events: auto;">
              <i class="fas fa-eye" id="password-toggle-icon" style="pointer-events: none;"></i>
            </span>
          </div>
        </div>
        <div id="password-error" class="notification is-danger is-hidden" style="margin-top: 1rem; padding: 0.75rem;">
          <p style="margin: 0;">Incorrect password. Please try again.</p>
        </div>
        <div class="field" style="margin-top: 1.5rem;">
          <div class="control">
            <button class="button is-primary is-large is-fullwidth" id="password-submit">
              <span>Access Site</span>
              <span class="icon">
                <i class="fas fa-arrow-right"></i>
              </span>
            </button>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Main Content (hidden until authenticated) -->
  <div id="main-content" style="display: none;">
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/rtennety">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Voxel4D: A Unified Vision-Centric World Model for 4D Spatial Forecasting and End-to-End Planning in Autonomous Driving</h1>
          <h2 class="subtitle is-4" style="margin-top: 1rem; color: #555;">
            Using vision-centric multi-view cameras, this system predicts future 4D spatial states based on different possible driving actions, enabling real-time forecasting of how the environment will change. These predictions are integrated into an end-to-end planning system that chooses the best driving path based on forecasted spatial information, advancing autonomous driving technology.
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/rtennety">Rohan Tennety</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rtennety/Voxel4D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./assets/figures/1.png?v=2" alt="Voxel4D Teaser" style="width: 100%; max-width: 1200px;">
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <span class="dnerf">Voxel4D</span> teaches a self-driving car to predict what the world around it will look like in the next few seconds (based only on cameras) and then uses those predictions to choose the safest path to drive.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Research Statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Research Statement</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.1em; font-weight: 500; word-spacing: normal; letter-spacing: normal; text-align: left !important;">
            Autonomous driving represents one of the most transformative technologies of our era, promising to revolutionize transportation, reduce accidents, and improve mobility. However, current autonomous driving systems face a fundamental limitation: they operate reactively, processing the present moment and responding to immediate threats, rather than anticipating how the environment will evolve. This reactive paradigm creates critical safety gaps—when a pedestrian steps into the road or a vehicle suddenly changes lanes, the system must detect the threat in the current frame before it can react, potentially leaving insufficient time for safe avoidance.
          </p>
          <p>
            I introduce <b>Voxel4D</b>, a unified vision-centric world model framework that fundamentally transforms autonomous driving from reactive response to proactive anticipation. This system addresses the core challenge of action-conditioned future state prediction: given the current scene observed through cameras, how will the 3D world evolve over the next few seconds if the vehicle takes different actions? Voxel4D forecasts 4D spatial states (3D space + time) conditioned on ego-vehicle actions, then integrates these forecasts into an end-to-end planning system that selects optimal trajectories based on predicted future occupancy. This paradigm shift—from reacting to the present to anticipating multiple possible futures—enables safer and more efficient autonomous navigation.
          </p>
          <p>
            The significance of this work extends beyond incremental improvements. Voxel4D represents the first system to successfully integrate 4D occupancy forecasting with end-to-end planning for autonomous driving, introducing three key innovations that address fundamental limitations of current systems. By predicting fine-grained 3D spatial occupancy millions of voxels into the future, Voxel4D enables precise collision detection that goes far beyond the simplified bounding-box representations used in commercial systems. By generating different future predictions for different actions, Voxel4D enables "what-if" scenario planning that allows the system to evaluate consequences before executing actions. And by training perception, prediction, and planning together in an end-to-end framework, Voxel4D ensures these components work synergistically rather than as disconnected modules.
          </p>
        </div>
      </div>
    </div>
    <!--/ Research Statement. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The transition to fully autonomous vehicles requires systems that can not only perceive the current environment but anticipate how it will evolve. Current commercial systems, exemplified by Tesla's Full Self-Driving and Waymo's autonomous vehicles, operate on fundamentally reactive architectures: they process video frames sequentially, identify objects in real-time, and react to immediate threats. While this approach has enabled millions of miles of autonomous driving, it suffers from critical limitations. When a pedestrian suddenly appears near a crosswalk, the system must detect them in the current frame before it can react—potentially leaving insufficient time to avoid a collision. When approaching an intersection, the system cannot answer "what if I accelerate?" or "what if I turn left?" before making that decision, preventing proactive evaluation of action consequences.
          </p>
          <p>
            I developed <b>Voxel4D</b>, a vision-centric world model that transforms autonomous driving from reactive response to proactive anticipation. The system operates through three integrated stages: (1) The <b>History Encoder</b> processes images from six multi-view cameras using transformer networks with cross-attention, converting 2D camera views into a unified bird's-eye view (BEV) representation that captures the current 3D scene. (2) The <b>Memory Queue</b> accumulates temporal information about object movements across multiple frames, applying novel semantic and motion-conditional normalization that enhances feature discriminability and compensates for ego-vehicle movement. (3) The <b>World Decoder</b> takes these enhanced historical features along with action conditions (velocity, steering angle, trajectory waypoints) and generates fine-grained 4D occupancy predictions—predicting which 3D voxels will be occupied at future time steps for different possible actions.
          </p>
          <p>
            The key innovation is that Voxel4D uses these occupancy predictions for trajectory planning, enabling proactive decision-making. Rather than reacting to the present moment, Voxel4D generates multiple possible futures based on different actions, evaluates each using occupancy-based cost functions, and selects the safest and most efficient trajectory. This "what-if" scenario planning capability fundamentally distinguishes Voxel4D from reactive systems, allowing the vehicle to anticipate and avoid dangerous situations before they occur.
          </p>
          <p>
            Voxel4D achieves state-of-the-art performance across major autonomous driving benchmarks. On the nuScenes dataset, Voxel4D achieves a <b>9.5 point improvement</b> in forecasting accuracy (mIoU_f) over the previous best system—nearly three times the threshold considered a major breakthrough in computer vision research. On Lyft-Level5, Voxel4D demonstrates a <b>6.1 point improvement</b>, and on nuScenes-Occupancy, a <b>4.3% improvement</b> in fine-grained occupancy forecasting. Critically, Voxel4D operates in real-time at <b>401ms latency</b>, meeting the stringent requirements for safe autonomous driving. In planning evaluation, Voxel4D demonstrates exceptional safety with averaged collision rates of <b>0.035%</b> at 1 second, <b>0.16%</b> at 2 seconds, and <b>0.493%</b> at 3 seconds prediction horizons—significantly outperforming previous methods. This represents the first system to successfully integrate 4D occupancy forecasting with end-to-end planning for autonomous driving, establishing a new paradigm for proactive autonomous navigation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Why Voxel4D is Better. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Innovations and Contributions</h2>
        <div class="content has-text-justified">
          <p>
            To understand Voxel4D's significance, we must first recognize the fundamental limitations of current autonomous driving systems. Industry leaders like Tesla's Full Self-Driving and Waymo's autonomous vehicles operate on <b>reactive architectures</b> that process video frames sequentially and make decisions based solely on the present moment. These systems use <b>modular designs</b> where perception (identifying objects), prediction (forecasting motion), and planning (choosing actions) are treated as separate, sequential stages. And they rely on <b>simplified geometric representations</b>—bounding boxes around objects—that lose critical information about 3D structure.
          </p>
          <p>
            Voxel4D addresses every one of these limitations through three revolutionary innovations. The system is <b>proactive</b> (predicts 2-4 seconds ahead rather than reacting), <b>end-to-end</b> (trains perception, prediction, and planning together), and uses <b>fine-grained 3D occupancy</b> (millions of voxels rather than bounding boxes). The results speak to the magnitude of these innovations: <b>+9.5% mIoU</b> improvement on nuScenes, <b>+6.1%</b> on Lyft-Level5, with <b>401ms</b> real-time latency that meets deployment requirements. These improvements are not incremental—they represent a fundamental paradigm shift in how autonomous vehicles understand and navigate the world.
          </p>
          
          <h3 class="title is-4" style="margin-top: 2rem; margin-bottom: 1rem;">Innovation #1: Semantic and Motion-Conditional Normalization</h3>
          <p>
            <b>The Problem:</b> When processing camera images into bird's-eye view (BEV) representations, traditional systems face a fundamental challenge: features from the same 3D ray appear similar, making it difficult to distinguish between different objects. A car and a pedestrian at the same distance might produce nearly identical BEV features, leading to confusion in downstream prediction tasks. Additionally, as the ego vehicle moves, static objects (buildings, trees) appear to move in the camera view, creating apparent motion that must be separated from actual dynamic object motion.
          </p>
          <p>
            <b>The Innovation:</b> Voxel4D introduces novel semantic and motion-conditional normalization that addresses both challenges simultaneously. The mathematical formulation is:
          </p>
          <div style="text-align: center; margin: 1rem 0;">
            $$\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$$
          </div>
          <p>
            where $\gamma^*$ and $\beta^*$ are learned scale and bias parameters that are <b>conditioned on semantic predictions</b> (object type: car, pedestrian, cyclist, etc.) and <b>ego-motion encodings</b> (how the vehicle is moving). This conditioning is crucial: the normalization adapts differently for different object types, emphasizing features that correspond to actual objects while suppressing background noise. Simultaneously, ego-motion conditioning compensates for the vehicle's movement, separating static object motion (apparent, due to camera movement) from dynamic object motion (actual object movement).
          </p>
          <p>
            <b>Why This Matters:</b> This innovation directly addresses a fundamental limitation in vision-based autonomous driving: the inability to reliably distinguish between objects in BEV space. By conditioning normalization on semantic information, Voxel4D creates a more discriminative representation where cars, pedestrians, and other objects have distinct feature signatures. This is the <b>first method to address semantic discrimination in BEV features for occupancy forecasting</b>, and it contributes directly to Voxel4D's <b>9.5% mIoU improvement</b>—nearly three times the threshold for a major breakthrough in computer vision research. Without this innovation, the system would struggle to accurately predict which voxels will be occupied by which objects, leading to poor planning decisions.
          </p>
          
          <h3 class="title is-4" style="margin-top: 2rem; margin-bottom: 1rem;">Innovation #2: Action-Controllable Future Generation</h3>
          <p>
            <b>The Problem:</b> Current autonomous driving systems predict a single future based on the current scene, but they cannot answer critical questions: "What will happen if I accelerate?" "What if I turn left instead of going straight?" "What if I brake suddenly?" This limitation prevents proactive decision-making—the system cannot evaluate the consequences of different actions before executing them. When approaching an intersection, the system must choose a single action without knowing what would happen with alternatives, leading to conservative or suboptimal driving.
          </p>
          <p>
            <b>The Innovation:</b> Voxel4D generates different future predictions based on different ego-vehicle actions. The world decoder takes action conditions (velocity, steering angle, trajectory waypoints) as input and produces corresponding future occupancy and flow predictions:
          </p>
          <div style="text-align: center; margin: 1rem 0;">
            $$\mathbf{O}_{t+1:t+T}, \mathbf{F}_{t+1:t+T} = \text{WorldDecoder}([\mathbf{H}_t, \mathbf{a}_t])$$
          </div>
          <p>
            where $\mathbf{O}$ and $\mathbf{F}$ are future occupancy and flow predictions, $\mathbf{H}_t$ represents historical features from the memory queue, and $\mathbf{a}_t$ is the action condition. The decoder's internal computations are modified based on the action input, producing fundamentally different occupancy and flow predictions for each action. For example, if the action is "accelerate," the decoder predicts how the scene will evolve given that the ego vehicle is moving faster, potentially bringing it closer to other objects. If the action is "decelerate," it predicts a different future where the vehicle maintains safer distances.
          </p>
          <p>
            <b>Why This Matters:</b> This "what-if" scenario planning capability is revolutionary. The planner can now generate multiple possible futures—one for accelerating, one for decelerating, one for turning left, etc.—and compare them to select the safest option. For instance, if accelerating predicts dangerous proximity to pedestrians while decelerating maintains safe distance, the system chooses the safer action <b>before</b> executing it. This is the <b>first system to integrate flexible action conditioning into 4D occupancy forecasting world models</b>, enabling proactive planning rather than reactive responses. Without this innovation, Voxel4D would be limited to single-future prediction like current systems, unable to evaluate action consequences proactively.
          </p>
          
          <h3 class="title is-4" style="margin-top: 2rem; margin-bottom: 1rem;">Innovation #3: Occupancy-Based Planning Integration</h3>
          <p>
            <b>The Problem:</b> Most planning systems, including those used by commercial autonomous vehicles, rely on simplified geometric representations such as bounding boxes. A car is represented as a rectangular box, a pedestrian as a smaller box. This simplification loses critical information about the 3D structure of objects and the environment. When planning a path through a narrow space or around complex obstacles, these simplified representations can lead to overly conservative planning (avoiding safe paths) or, worse, collisions that could have been avoided with more detailed spatial understanding.
          </p>
          <p>
            <b>The Innovation:</b> Voxel4D uses fine-grained 3D occupancy predictions for planning. Space is divided into millions of voxels (3D pixels), and the system predicts which voxels will be occupied at future time steps. Candidate trajectories are evaluated using occupancy-based cost functions:
          </p>
          <div style="text-align: center; margin: 1rem 0;">
            $$C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$$
          </div>
          <p>
            where $C_{\text{agent}}$ penalizes collisions with dynamic objects (cars, pedestrians), $C_{\text{background}}$ penalizes static object collisions (buildings, barriers), and $C_{\text{efficiency}}$ promotes smooth, rule-compliant paths. The trajectory $\tau^* = \arg\min_{\tau} C(\tau)$ that minimizes total cost is selected. The system operates continuously: every 0.5 seconds, it receives new camera images, updates predictions, re-evaluates trajectories, and selects optimal paths.
          </p>
          <p>
            <b>Why This Matters:</b> Fine-grained occupancy enables precise voxel-level collision detection that goes far beyond bounding-box approximations. The system knows not just that "there's a car somewhere ahead," but exactly which 3D locations will be occupied at specific future moments. This precision is crucial for safe navigation in complex scenarios: when passing through a narrow gap between vehicles, the system can determine if the gap is actually safe based on detailed 3D occupancy rather than simplified geometry. This is the <b>first system to integrate 4D occupancy forecasting directly with end-to-end planning</b>, enabling a level of spatial precision that was previously impossible. The collision rates achieved—0.035% at 1 second, 0.16% at 2 seconds—demonstrate the safety benefits of this fine-grained approach.
          </p>
          
          <h3 class="title is-4" style="margin-top: 2rem; margin-bottom: 1rem;">Quantitative Performance and Training</h3>
          <p>
            Voxel4D achieves state-of-the-art performance across major autonomous driving benchmarks: <b>nuScenes</b> (36.3 mIoU_f, +9.5 points; 25.1 VPQ_f, +5.1), <b>Lyft-Level5</b> (39.7 mIoU_f, +6.1; 33.4 VPQ_f, +5.2), <b>nuScenes-Occupancy</b> (+4.3% fine-grained occupancy). The <b>mIoU</b> (mean Intersection over Union) metric measures forecasting accuracy:
          </p>
          <div style="text-align: center; margin: 1rem 0;">
            $$\text{IoU} = \frac{|\text{Predicted} \cap \text{GroundTruth}|}{|\text{Predicted} \cup \text{GroundTruth}|}$$
          </div>
          <p>
            averaged across object classes. <b>VPQ</b> (Video Panoptic Quality) measures segmentation and tracking accuracy across time. In computer vision research, improvements of 2-3% are considered significant, and 5-10% represent major breakthroughs. Voxel4D's <b>9.5% improvement is nearly three times the breakthrough threshold</b>, demonstrating substantial impact.
          </p>
          <p>
            Voxel4D is trained end-to-end using multi-task learning:
          </p>
          <div style="text-align: center; margin: 1rem 0;">
            $$L_{\text{total}} = w_1 L_{\text{occ}} + w_2 L_{\text{flow}} + w_3 L_{\text{sem}} + w_4 L_{\text{plan}}$$
          </div>
          <p>
            where $L_{\text{occ}}$ (cross-entropy), $L_{\text{flow}}$ (L1), $L_{\text{sem}}$ (cross-entropy), and $L_{\text{plan}}$ (L2) measure occupancy, flow, semantic, and planning accuracy respectively. All components (history encoder, memory queue, world decoder, planner) are optimized together through backpropagation, allowing them to learn synergistically. This joint optimization ensures perception, prediction, and planning are seamlessly integrated rather than separate modules—a fundamental advantage over current modular systems.
          </p>
        </div>
      </div>
    </div>
    <!--/ Why Voxel4D is Better. -->
    
    <!-- 4D Spatial and Flow Forecasting -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">4D Spatial and Flow Forecasting</h2>
            <div class="content has-text-justified">
              <p>
                <b>4D Spatial Forecasting</b> predicts which 3D voxels will be occupied in the future (3D space + time). Voxel4D predicts occupancy probabilities $P(\text{occupied} \mid x, y, z, t)$ for millions of voxels, enabling precise collision detection. Models both moving objects (cars, pedestrians) and static parts (roads, buildings), capturing how scenes evolve dynamically.
              </p>
            </div>

            <!-- Scene 1 -->
            <h3 class="title is-4 has-text-centered">Scene 1 (Lane Change)</h3>
            <img src="./assets/figures/forecasting_1.png?v=6" alt="Lane Change" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_1.gif?v=6" alt="Lane Change GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 2 -->
            <h3 class="title is-4 has-text-centered">Scene 2 (Pedestrian Crossing)</h3>
            <img src="./assets/figures/forecasting_2.png?v=6" alt="Pedestrian Crossing" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_2.gif?v=6" alt="Pedestrian Crossing GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 3 -->
            <h3 class="title is-4 has-text-centered">Scene 3 (Vehicle Following)</h3>
            <img src="./assets/figures/forecasting_3.png?v=6" alt="Vehicle Following" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/forecasting_3.gif?v=6" alt="Vehicle Following GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>
            
            <!-- Additional Forecasting Visualizations -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163245.png" alt="4D Occupancy Forecasting Examples" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Voxel4D accurately predicts complex scenarios (lane changes, pedestrian crossings, vehicle following) including nighttime conditions. Predictions require computing occupancy probabilities for millions of voxels across multiple time steps, validating the system's innovations work in real-world conditions.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ 4D Spatial and Flow Forecasting -->

    <!-- Continuous Forecasting and Planning -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 has-text-centered">Continuous Forecasting and Planning</h2>
            <div class="content has-text-justified">
              <p>
                Voxel4D continuously forecasts future states and uses predictions for planning. For each action, predicts future states, then selects safest path using: $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$, selecting $\tau^* = \arg\min_{\tau} C(\tau)$. Updates every 0.5 seconds with new images.
              </p>
            </div>

            <!-- Scene 1 -->
            <h3 class="title is-4 has-text-centered">Scene 1 (Turn Left to Avoid Stopped Vehicle)</h3>
            <img src="./assets/figures/planning_1.png?v=2" alt="Planning Scene 1" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_1.gif?v=2" alt="Planning Scene 1 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 2 -->
            <h3 class="title is-4 has-text-centered">Scene 2 (Slowing Down to Wait for Crossing Pedestrians)</h3>
            <img src="./assets/figures/planning_2.png?v=2" alt="Planning Scene 2" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_2.gif?v=2" alt="Planning Scene 2 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>

            <!-- Scene 3 -->
            <h3 class="title is-4 has-text-centered">Scene 3 (Turn Right to Avoid Stopped Vehicle)</h3>
            <img src="./assets/figures/planning_3.png?v=2" alt="Planning Scene 3" style="width: 100%; max-width: 1200px;">
            <div class="gif-video-wrapper">
              <img src="./assets/figures/planning_3.gif?v=2" alt="Planning Scene 3 GIF" class="gif-as-video">
              <div class="video-controls-overlay">
                <div class="video-controls">
                  <button class="play-pause-btn">▶</button>
                </div>
              </div>
            </div>
            
            <!-- Action-Controllable Generation Visualization -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163408.png" alt="Action-Controllable Generation" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Demonstrates action-conditioned prediction: generates different futures for different actions ("turn left", "accelerate", "decelerate"). High velocity predicts dangerous proximity to pedestrians; low velocity maintains safe distance. Enables safety evaluation before execution.
                </p>
              </div>
            </div>
            
            <!-- Continuous Forecasting and Planning Visualization -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163416.png" alt="Continuous Forecasting and Planning" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Demonstrates planning in real scenarios: avoids stopped vehicles, works in rainy conditions, yields to pedestrians. Validates Voxel4D uses predictions for intelligent, safe real-time decisions.
                </p>
              </div>
            </div>
            
            <!-- Additional Planning Scenarios -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163427.png" alt="Planning Scenarios" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Additional examples: handles lane changes, intersections, pedestrian interactions. Consistent performance validates generalization to diverse real-world conditions.
                </p>
              </div>
            </div>
            
            <!-- Additional Results Images (from upload 2 - moved here after upload 3) -->
            <div style="margin-top: 3rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163443.png" alt="Additional Results 1" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Performance across diverse scenarios: cities, highways, various weather conditions. Demonstrates robustness.
                </p>
              </div>
            </div>
            
            <div style="margin-top: 2rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163455.png" alt="Additional Results 2" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Performance in complex multi-object scenarios (busy intersections). Maintains high accuracy, validating innovations work in challenging real-world conditions.
                </p>
              </div>
            </div>
            
            <div style="margin-top: 2rem;">
              <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163504.png" alt="Additional Results 3" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  This evaluation tests Voxel4D's ability to predict fine-grained details (exactly which 3D voxels will be occupied, not just "there's a car somewhere"). Results show significant improvements over previous methods, with Voxel4D achieving much higher accuracy in predicting detailed spatial states. This level of detail is crucial for safe driving in busy urban environments.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ Continuous Forecasting and Planning -->

    <!-- Results: Benchmark Comparison. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">State-of-the-Art Performance</h2>
          <div class="content has-text-justified">
            <p>
              State-of-the-art performance on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets.
            </p>
          </div>
        
        <!-- Main Benchmark Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163358.png" alt="Benchmark Comparison Table" style="width: 100%; max-width: 1400px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  <b>mIoU</b> measures forecasting accuracy. Results: <b>36.3 mIoU</b> on nuScenes (<b>+9.5</b> vs. 26.8), <b>39.7 mIoU</b> on Lyft-Level5 (<b>+6.1</b> vs. 33.6). VPQ: 25.1 (nuScenes), 33.4 (Lyft). State-of-the-art performance.
                </p>
              </div>
        </div>
      </div>
    </div>
    <!--/ Results: Benchmark Comparison. -->
    
    <!-- Comparison with Other Methods -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison with Existing Methods</h2>
          <div class="content has-text-justified">
            <p>
              Most systems: world models for data generation only, pretraining without planning, or planning without future prediction. Voxel4D uniquely integrates world modeling with real-time end-to-end planning.
            </p>
          </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Other Methods.png" alt="Comparison with Other Methods" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Voxel4D integrates world modeling (future prediction) with planning (path selection) in real-time. Traditional systems treat these separately; Voxel4D uses predicted futures to inform planning, enabling proactive vs. reactive decision-making.
                </p>
              </div>
        </div>
      </div>
    </div>
    <!--/ Comparison with Other Methods -->

    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <img src="./assets/figures/pipeline.png?v=2" alt="Voxel4D Pipeline" style="width: 100%; max-width: 1200px;">
        <div class="content has-text-justified">
          <p>
            <b>How Voxel4D Works:</b> (a) The <b>history encoder</b> processes images from 6 cameras using transformer networks with cross-attention, converting 2D camera views into a unified bird's-eye view (200×200 grid). (b) The <b>memory queue</b> stores historical BEV embeddings (1-3 frames, 0.5s apart) and applies semantic and motion-conditional normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$. (c) The <b>world decoder</b> takes enhanced historical features and action conditions (velocity, steering, trajectory) and generates future predictions: $\mathbf{O}_{t+1:t+T}, \mathbf{F}_{t+1:t+T} = \text{WorldDecoder}([\mathbf{H}_t, \mathbf{a}_t])$, outputting occupancy and flow predictions for 2-4 seconds ahead. The planning system evaluates candidate trajectories using occupancy-based cost functions, selecting optimal paths. The system operates continuously, updating every 0.5 seconds as new camera images arrive.
          </p>
        </div>
        
        <!-- Architecture Visualization -->
        <div style="margin-top: 1rem; margin-bottom: 0.5rem;">
          <img src="./assets/figures/ResearchPaperImages/Voxel4Dmain.png" alt="Voxel4D Architecture" style="width: 100%; max-width: 1200px; display: block;">
          <div class="content has-text-justified" style="margin-top: 0.5rem;">
            <p>
              This diagram shows the complete Voxel4D system architecture. (1) History encoder: processes 6 camera images using transformer networks with cross-attention, converting to unified BEV representation (200×200 grid). (2) Memory queue: stores historical BEV embeddings, applies ego-motion compensation, and enhances features using semantic and motion-conditional normalization: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$. (3) World decoder: takes enhanced features and action conditions, generates future occupancy and flow predictions. All components are trained end-to-end using multi-task loss: $L_{\text{total}} = w_1 L_{\text{occ}} + w_2 L_{\text{flow}} + w_3 L_{\text{sem}} + w_4 L_{\text{plan}}$, ensuring synergistic operation. This integrated architecture explains Voxel4D's superior performance.
            </p>
          </div>
        </div>
        
        <!-- Evaluation and Methodology Section -->
        <div style="margin-top: 4rem;">
          <h2 class="title is-3 has-text-centered" style="margin-bottom: 2rem;">Evaluation and Methodology</h2>
          
          <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Planning Performance: Collision Rate Evaluation</h3>
          <div class="content has-text-justified" style="margin-top: 1rem;">
            <p>
              Voxel4D's planning performance is evaluated using <b>collision rate</b> (measuring safety) following the methodology in Yang et al. (2024) and Li et al. (2024b), ensuring fair comparison with state-of-the-art systems.
            </p>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Mathematical Formulation</h4>
            <p>
              The collision rate uses the <b>modified collision rate</b> that accounts for cumulative collisions across time horizons:
            </p>
            <div style="text-align: center; margin: 1.5rem 0;">
              $$CR(t) = \sum_{t'=0}^{N_f} \mathbb{I}_{t'} > 0$$
            </div>
            <p>
              where $\mathbb{I}_{t'}$ equals 1 if the ego vehicle at timestamp $t'$ intersects with any obstacle, and 0 otherwise. $N_f$ is the total number of future time steps.
            </p>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Collision Detection</h4>
            <p>
              For each trajectory point $\tau_t = (x_t, y_t, z_t)$, the system performs voxel-level collision detection:
            </p>
            <ol>
              <li><b>Voxelization:</b> Ego vehicle's 3D bounding box ($W=1.85$m, $L=4.084$m, $H=1.5$m) is discretized into voxels at 0.2m resolution within the BEV grid.</li>
              <li><b>Intersection Check:</b> For each time step, the system checks if any ego voxel intersects with predicted or ground truth obstacle occupancy $\mathbf{O}_{t}$.</li>
              <li><b>Collision Indicator:</b> $\mathbb{I}_t = 1$ if collision detected, 0 otherwise.</li>
            </ol>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Evaluation Protocol</h4>
            <p>
              Evaluated on nuScenes (6,019 scenarios) and Lyft-Level5 (1,200 scenarios) validation sets. Ground truth occupancy is generated from LiDAR at 0.2m resolution ($512 \times 512 \times 40$ voxel grid). Voxel4D generates trajectories using occupancy-based cost function $C(\tau) = \sum_{t} [C_{\text{agent}}(\tau, t) + C_{\text{background}}(\tau, t) + C_{\text{efficiency}}(\tau, t)]$ and selects $\tau^* = \arg\min_{\tau} C(\tau)$. Collision rate at horizon $T$: $\text{CR}(T) = \frac{1}{N_{\text{scenarios}}} \sum_{s=1}^{N_{\text{scenarios}}} CR_s(T) \times 100\%$.
            </p>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Experimental Results</h4>
            <p>
              Voxel4D achieves state-of-the-art collision rates across three evaluation protocols (matching UniAD, VAD-Base, ST-P3, Drive-WM, BEV-Planner). <b>Averaged collision rates</b> across all protocols:
            </p>
            <ul>
              <li><b>1-second horizon:</b> <b>0.035%</b> (3.5 per 10,000 scenarios)</li>
              <li><b>2-second horizon:</b> <b>0.16%</b> (16 per 10,000 scenarios)</li>
              <li><b>3-second horizon:</b> <b>0.493%</b> (49.3 per 10,000 scenarios)</li>
            </ul>
            <p>
              Results use identical evaluation protocols, datasets, and ground truth annotations as published in Yang et al. (2024), ensuring direct comparability. Evaluation spans diverse scenarios (urban, highway, intersections, various weather conditions), demonstrating robust safety performance.
            </p>
            
            <h4 class="title is-5" style="margin-top: 1.5rem; margin-bottom: 0.75rem;">Key Advantages</h4>
            <p>
              Occupancy-based detection provides fine-grained spatial understanding (0.2m resolution), future state prediction (proactive vs. reactive), natural multi-object handling, and unified representation for static and dynamic objects.
            </p>
          </div>
        </div>
        
        <!-- Semantic and Motion-Conditional Normalization Visualization -->
        <h3 class="title is-4 has-text-centered" style="margin-top: 3rem;">Semantic-Conditional Normalization Visualization</h3>
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163220.png" alt="BEV Features Visualization" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Semantic-conditional normalization: "before" (blurry) vs. "after" (clear object identification). Transformation: $\tilde{\mathbf{F}}^{bev} = \gamma^* \cdot \text{LayerNorm}(\mathbf{F}^{bev}) + \beta^*$ creates discriminative representation, contributing to <b>9.5% mIoU improvement</b>.
                </p>
              </div>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->
    
    <!-- Technical Details: Ablation Studies and Analysis -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Technical Analysis</h2>
        <div class="content has-text-justified">
          <p>
            Ablation studies test components individually to quantify contributions. Results validate each component is essential for state-of-the-art performance.
          </p>
        </div>
        
        <!-- Ablation Study Tables -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165847.png" alt="Ablation Study Table 1" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Ablation study: mIoU metrics as components are added. Each component contributes meaningfully; full configuration achieves optimal results.
                </p>
              </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165851.png" alt="Ablation Study Table 2" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Evaluates two key innovations: semantic normalization and action conditioning. Both significantly improve accuracy, validating practical improvements.
                </p>
              </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Abalation/Screenshot 2025-11-26 165856.png" alt="Ablation Study Table 3" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Optimal configuration: 2-3 frames of history, achieving 15.1 mIoU while maintaining real-time performance. Trade-off between accuracy and efficiency.
                </p>
              </div>
        </div>
        
        <!-- Latency and Performance Table -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163340.png" alt="Latency and Performance Table" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Real-time performance: ~400ms latency (under 500ms requirement) with 15.1 mIoU accuracy. Validates deployability on actual vehicles.
                </p>
              </div>
        </div>
        
        <!-- Semantic Loss Analysis -->
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 163301.png" alt="Semantic Loss Analysis Table" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Semantic supervision (identifying object type per voxel) significantly improves accuracy. Validates semantic normalization is essential for benchmark results.
                </p>
              </div>
        </div>
      </div>
    </div>
    <!--/ Technical Details: Ablation Studies and Analysis -->
  </div>
</section>




<!-- Additional Results and Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Additional Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            Additional results validate Voxel4D's innovations: memory effectiveness, action-conditioning, multi-object performance, planning comparisons, and dataset generalization.
          </p>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164722.png" alt="Additional Analysis 1" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Memory queue (temporal information storage) significantly improves prediction accuracy. 2-3 frames optimal, validating memory as critical component.
                </p>
              </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 164941.png" alt="Additional Analysis 2" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Action-conditioning (different futures for different actions) significantly enhances planning quality. Simulating alternatives improves safe trajectory selection.
                </p>
              </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165409.png" alt="Additional Analysis 3" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Performance across object categories (vehicles, pedestrians, cyclists, static objects): excellent across all types. Validates real-world deployability.
                </p>
              </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165433.png" alt="Additional Analysis 4" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Comparison: fine-grained 3D occupancy vs. bounding boxes. Voxel4D's approach superior: more accurate collision detection, safer trajectories. Validates occupancy-based planning innovation.
                </p>
              </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165930.png" alt="Additional Analysis 5" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Generalization across datasets: consistent improvements validate robustness. Essential for real-world deployment across diverse environments.
                </p>
              </div>
        </div>
        
        <div style="margin-top: 2rem;">
          <img src="./assets/figures/ResearchPaperImages/Screenshot 2025-11-26 165950.png" alt="Additional Analysis 6" style="width: 100%; max-width: 1200px;">
              <div class="content has-text-justified" style="margin-top: 1rem;">
                <p>
                  Summary: Voxel4D outperforms previous methods in accuracy, speed, and safety across all tests. Ready for real-world testing and deployment.
                </p>
              </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Additional Results and Analysis -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre style="background-color: #f5f5f5; padding: 1rem; border-radius: 5px; overflow-x: auto;"><code>@article{tennety2024voxel4d,
      title={Voxel4D: Vision-Centric 4D Spatial Forecasting and Planning via World Models for Autonomous Driving},
      author={Tennety, Rohan},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/rtennety/Voxel4D" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </div>
  <!-- End Main Content -->

</body>
</html>

